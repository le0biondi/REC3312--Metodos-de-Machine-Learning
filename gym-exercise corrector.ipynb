{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde01d2f-77b1-44ab-8cf0-636e042d870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Nenhuma GPU NVIDIA detectada ou configurada. Todas as opera√ß√µes de Machine Learning utilizar√£o a CPU.\n",
      "‚úÖ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\n",
      "‚úÖ Fun√ß√µes de extra√ß√£o de pose, c√°lculo de √¢ngulos e gera√ß√£o de erros sint√©ticos prontas.\n",
      "‚ÑπÔ∏è Dataset 'workout-exercises-images' j√° existe em workout-exercises-images. Pulando download.\n",
      "\n",
      "‚úÖ Tipos de exerc√≠cios encontrados no dataset: ['barbell biceps curl', 'bench press', 'chest fly machine', 'deadlift', 'decline bench press', 'hammer curl', 'hip thrust', 'incline bench press', 'lat pulldown', 'lateral raises', 'leg extension', 'leg raises', 'plank', 'pull up', 'push up', 'romanian deadlift', 'russian twist', 'shoulder press', 'squat', 't bar row', 'tricep dips', 'tricep pushdown']\n",
      "\n",
      "‚öôÔ∏è Processando 705 imagens para o exerc√≠cio: barbell biceps curl\n",
      "   Amostrando 400 imagens de 705 dispon√≠veis para barbell biceps curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de barbell biceps curl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 625 imagens para o exerc√≠cio: bench press\n",
      "   Amostrando 400 imagens de 625 dispon√≠veis para bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de bench press: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:19<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 527 imagens para o exerc√≠cio: chest fly machine\n",
      "   Amostrando 400 imagens de 527 dispon√≠veis para chest fly machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de chest fly machine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 530 imagens para o exerc√≠cio: deadlift\n",
      "   Amostrando 400 imagens de 530 dispon√≠veis para deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de deadlift: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 514 imagens para o exerc√≠cio: decline bench press\n",
      "   Amostrando 400 imagens de 514 dispon√≠veis para decline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de decline bench press: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:17<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 546 imagens para o exerc√≠cio: hammer curl\n",
      "   Amostrando 400 imagens de 546 dispon√≠veis para hammer curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hammer curl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 557 imagens para o exerc√≠cio: hip thrust\n",
      "   Amostrando 400 imagens de 557 dispon√≠veis para hip thrust.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hip thrust: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 729 imagens para o exerc√≠cio: incline bench press\n",
      "   Amostrando 400 imagens de 729 dispon√≠veis para incline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de incline bench press: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:20<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 646 imagens para o exerc√≠cio: lat pulldown\n",
      "   Amostrando 400 imagens de 646 dispon√≠veis para lat pulldown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lat pulldown: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:20<00:00, 19.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 843 imagens para o exerc√≠cio: lateral raises\n",
      "   Amostrando 400 imagens de 843 dispon√≠veis para lateral raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lateral raises: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 586 imagens para o exerc√≠cio: leg extension\n",
      "   Amostrando 400 imagens de 586 dispon√≠veis para leg extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg extension: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:20<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 514 imagens para o exerc√≠cio: leg raises\n",
      "   Amostrando 400 imagens de 514 dispon√≠veis para leg raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg raises: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 993 imagens para o exerc√≠cio: plank\n",
      "   Amostrando 400 imagens de 993 dispon√≠veis para plank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de plank: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 615 imagens para o exerc√≠cio: pull up\n",
      "   Amostrando 400 imagens de 615 dispon√≠veis para pull up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de pull up: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 601 imagens para o exerc√≠cio: push up\n",
      "   Amostrando 400 imagens de 601 dispon√≠veis para push up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de push up: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 555 imagens para o exerc√≠cio: romanian deadlift\n",
      "   Amostrando 400 imagens de 555 dispon√≠veis para romanian deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de romanian deadlift: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:20<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 522 imagens para o exerc√≠cio: russian twist\n",
      "   Amostrando 400 imagens de 522 dispon√≠veis para russian twist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de russian twist: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 512 imagens para o exerc√≠cio: shoulder press\n",
      "   Amostrando 400 imagens de 512 dispon√≠veis para shoulder press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de shoulder press: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 742 imagens para o exerc√≠cio: squat\n",
      "   Amostrando 400 imagens de 742 dispon√≠veis para squat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de squat: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 668 imagens para o exerc√≠cio: t bar row\n",
      "   Amostrando 400 imagens de 668 dispon√≠veis para t bar row.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de t bar row: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 698 imagens para o exerc√≠cio: tricep dips\n",
      "   Amostrando 400 imagens de 698 dispon√≠veis para tricep dips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep dips: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Processando 625 imagens para o exerc√≠cio: tricep pushdown\n",
      "   Amostrando 400 imagens de 625 dispon√≠veis para tricep pushdown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep pushdown: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:21<00:00, 18.52it/s]\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total de 7852 amostras processadas.\n",
      "   Shape das features (X): (7852, 8)\n",
      "   Shape dos r√≥tulos de exerc√≠cio (y_exercise): (7852, 22)\n",
      "   Shape dos r√≥tulos de forma (y_form): (7852, 2)\n",
      "   Exerc√≠cios detectados: ['barbell biceps curl' 'bench press' 'chest fly machine' 'deadlift'\n",
      " 'decline bench press' 'hammer curl' 'hip thrust' 'incline bench press'\n",
      " 'lat pulldown' 'lateral raises' 'leg extension' 'leg raises' 'plank'\n",
      " 'pull up' 'push up' 'romanian deadlift' 'russian twist' 'shoulder press'\n",
      " 'squat' 't bar row' 'tricep dips' 'tricep pushdown']\n",
      "‚úÖ exercise_encoder salvo em models\\exercise_encoder.pkl\n",
      "‚úÖ feature_scaler salvo em models\\feature_scaler.pkl\n",
      "‚úÖ Pr√©-processamento de dados de exerc√≠cios conclu√≠do e escalers salvos.\n",
      "‚ÑπÔ∏è Modelo MLP pr√©-treinado encontrado. Carregando...\n",
      "‚úÖ Modelo MLP carregado.\n",
      "Avaliando o modelo MLP no conjunto de valida√ß√£o...\n",
      "MLP Keras - Perda de Valida√ß√£o: 1.1351, Acur√°cia Exerc√≠cio de Valida√ß√£o: 0.5977, Acur√°cia Forma de Valida√ß√£o: 0.6155\n",
      "‚öôÔ∏è Treinando modelo Scikit-learn: Random Forest\n",
      "   Acur√°cia de Valida√ß√£o para Random Forest: 0.6595\n",
      "‚úÖ Modelo 'Random Forest' treinado e salvo em models\\best_random_forest_model.pkl\n",
      "‚öôÔ∏è Treinando modelo Scikit-learn: Regress√£o Log√≠stica\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Acur√°cia de Valida√ß√£o para Regress√£o Log√≠stica: 0.3393\n",
      "‚úÖ Modelo 'Regress√£o Log√≠stica' treinado e salvo em models\\best_regress√£o_log√≠stica_model.pkl\n",
      "\n",
      "--- Resultados da Compara√ß√£o de Modelos (Valida√ß√£o) ---\n",
      "Modelo: MLP Keras (Exerc√≠cio & Forma)\n",
      "  Perda de Valida√ß√£o: 1.1351\n",
      "  Acur√°cia de Exerc√≠cio de Valida√ß√£o: 0.5977\n",
      "  Acur√°cia de Forma de Valida√ß√£o: 0.6155\n",
      "Modelo: Random Forest (Apenas Exerc√≠cio)\n",
      "  Acur√°cia de Exerc√≠cio de Valida√ß√£o: 0.6595\n",
      "Modelo: Regress√£o Log√≠stica (Apenas Exerc√≠cio)\n",
      "  Acur√°cia de Exerc√≠cio de Valida√ß√£o: 0.3393\n",
      "\n",
      "üèÜ Modelo com maior acur√°cia para Classifica√ß√£o de Exerc√≠cio (com base na acur√°cia de valida√ß√£o): 'Random Forest (Apenas Exerc√≠cio)' com acur√°cia: 0.6595\n",
      "\n",
      "Escolha final para aplica√ß√£o em tempo real (devido √† sa√≠da combinada de exerc√≠cio e forma): 'MLP Keras (Exerc√≠cio & Forma)'\n",
      "Carregando este modelo para detec√ß√£o em tempo real: models\\best_mlp_model.h5\n",
      "\n",
      "‚úÖ Webcam ativada. Pressione 'Q' para sair da janela de visualiza√ß√£o.\n",
      "UTC: 09/07/2025 03:05:03 (UTC)\n",
      "Bras√≠lia: 08/07/2025 21:05:03 (UTC-3)\n",
      "\n",
      "‚ö†Ô∏è A performance pode ser limitada pela sua CPU. Para melhor experi√™ncia, considere GPUs dedicadas.\n",
      "‚úÖ Programa finalizado. Recursos liberados.\n"
     ]
    }
   ],
   "source": [
    "# gym_exercise_corrector.py\n",
    "\n",
    "# 1. Instala√ß√£o das Depend√™ncias\n",
    "# Garantir que todas as bibliotecas necess√°rias estejam instaladas.\n",
    "# Adicionamos 'mediapipe' que √© essencial para a estimativa de pose.\n",
    "# O pacote 'tensorflow' por padr√£o instala a vers√£o otimizada para CPU.\n",
    "\n",
    "# Removido 'tensorflow-gpu' e 'gdown' j√° que estamos em CPU e o dataset √© baixado com kaggle cli\n",
    "# A linha abaixo deve ser executada uma vez no terminal ou na c√©lula de notebook para instalar/atualizar\n",
    "# !pip install tensorflow numpy pandas matplotlib requests scikit-learn imbalanced-learn tqdm kaggle mediapipe opencv-python --upgrade\n",
    "# !pip install --upgrade h5py Keras\n",
    "\n",
    "# 2. Configura√ß√µes Iniciais e Verifica√ß√£o de Hardware\n",
    "# Importa√ß√µes essenciais para o projeto.\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import sys # Importado para sys.exit()\n",
    "\n",
    "# Importar MediaPipe para estimativa de pose\n",
    "import mediapipe as mp\n",
    "\n",
    "# Importar bibliotecas para modelos Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib # Para salvar/carregar scalers e modelos Scikit-learn\n",
    "\n",
    "# Configura√ß√£o de CPU\n",
    "# Em um ambiente com apenas CPU, o TensorFlow ir√° automaticamente utilizar o processador.\n",
    "print(\"‚ÑπÔ∏è Nenhuma GPU NVIDIA detectada ou configurada. Todas as opera√ß√µes de Machine Learning utilizar√£o a CPU.\")\n",
    "\n",
    "# Configurar credenciais do Kaggle\n",
    "# Isso √© crucial para baixar o dataset de exerc√≠cios automaticamente.\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Voc√™ pode obter isso no seu perfil do Kaggle, na se√ß√£o \"API\".\n",
    "kaggle_creds = {\n",
    "    \"username\": \"SEU_USERNAME_KAGGLE\", # Substitua pelo seu username do Kaggle\n",
    "    \"key\": \"SUA_CHAVE_KAGGLE\" # Substitua pela sua chave API do Kaggle\n",
    "}\n",
    "\n",
    "# Salvar credenciais no arquivo kaggle.json\n",
    "with open(kaggle_dir / 'kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "\n",
    "# Adicionar ao PATH para que o comando 'kaggle' possa ser executado\n",
    "# Isso √© importante para que o subprocess.run possa encontrar o comando kaggle.\n",
    "os.environ['PATH'] += os.pathsep + str(Path.home() / '.local' / 'bin')\n",
    "\n",
    "print(\"‚úÖ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\")\n",
    "\n",
    "\n",
    "# 3. Defini√ß√£o de Fun√ß√µes Auxiliares para Extra√ß√£o de Features (MediaPipe) e Gera√ß√£o de Erros\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detec√ß√£o de landmarks.\n",
    "# Static_image_mode=False √© para processamento de v√≠deo, True para imagens est√°ticas.\n",
    "# Min_detection_confidence e min_tracking_confidence s√£o limiares de confian√ßa.\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles # Corrigido aqui, estava mp.solutions.drawing_utils novamente\n",
    "\n",
    "# Fun√ß√£o para calcular o √¢ngulo entre tr√™s pontos (landmarks)\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calcula o √¢ngulo em graus entre tr√™s pontos (landmarks).\n",
    "    O ponto 'b' √© o v√©rtice do √¢ngulo.\n",
    "    Args:\n",
    "        a (list/tuple): Coordenadas (x, y) ou (x, y, z) do primeiro ponto.\n",
    "        b (list/tuple): Coordenadas (x, y) ou (x, y, z) do ponto central (v√©rtice).\n",
    "        c (list/tuple): Coordenadas (x, y) ou (x, y, z) do terceiro ponto.\n",
    "    Returns:\n",
    "        float: O √¢ngulo em graus.\n",
    "    \"\"\"\n",
    "    a = np.array(a) # Primeiro ponto\n",
    "    b = np.array(b) # Ponto do v√©rtice\n",
    "    c = np.array(c) # Terceiro ponto\n",
    "\n",
    "    # Vetores formados pelos pontos\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # Produto escalar e normas dos vetores\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    # Garantir que o valor esteja dentro do dom√≠nio de arccos para evitar erros de floating point\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Fun√ß√£o para extrair features de pose de uma imagem (√¢ngulos e dist√¢ncias)\n",
    "def extract_pose_features(image_path, pose_detector):\n",
    "    \"\"\"\n",
    "    Extrai um vetor de features de pose (√¢ngulos chave do corpo) de uma imagem.\n",
    "    Args:\n",
    "        image_path (str): Caminho para a imagem.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado.\n",
    "    Returns:\n",
    "        numpy.ndarray or None: Um vetor numpy de features (√¢ngulos), ou None se nenhuma pose for detectada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lendo a imagem\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # print(f\"‚ö†Ô∏è N√£o foi poss√≠vel carregar a imagem: {image_path}\") # Comentado para evitar poluir o log\n",
    "            return None\n",
    "\n",
    "        # Convertendo a imagem de BGR para RGB (MediaPipe espera RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Processando a imagem para estimativa de pose\n",
    "        results = pose_detector.process(image_rgb)\n",
    "\n",
    "        # Se houver landmarks de pose detectadas\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Mapeamento das landmarks do MediaPipe para f√°cil acesso\n",
    "            # Para o `mp_pose.PoseLandmark` o `.value` √© necess√°rio para acessar o √≠ndice correto\n",
    "            \n",
    "            # √Çngulos dos cotovelos (ombro, cotovelo, punho)\n",
    "            left_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            )\n",
    "            right_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            )\n",
    "\n",
    "            # √Çngulos dos ombros (quadril, ombro, cotovelo)\n",
    "            left_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            )\n",
    "            right_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            )\n",
    "            \n",
    "            # √Çngulos dos joelhos (quadril, joelho, tornozelo)\n",
    "            left_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            )\n",
    "            right_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            )\n",
    "\n",
    "            # √Çngulos dos quadris (ombro, quadril, joelho) - essencial para postura de tronco\n",
    "            left_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "            )\n",
    "            right_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            )\n",
    "            \n",
    "            # Coletar todas as features em um vetor\n",
    "            features = [\n",
    "                left_elbow_angle, right_elbow_angle,\n",
    "                left_shoulder_angle, right_shoulder_angle,\n",
    "                left_knee_angle, right_knee_angle,\n",
    "                left_hip_angle, right_hip_angle\n",
    "            ]\n",
    "            \n",
    "            return np.array(features)\n",
    "        else:\n",
    "            return None # Nenhuma pose detectada\n",
    "    except Exception as e:\n",
    "        # print(f\"‚ö†Ô∏è Erro ao extrair features de {image_path}: {e}\") # Comentado para evitar poluir o log\n",
    "        return None\n",
    "\n",
    "# Nova fun√ß√£o para gerar formas incorretas sinteticamente\n",
    "def synthesize_incorrect_form(original_features, exercise_type_str):\n",
    "    \"\"\"\n",
    "    Aplica uma perturba√ß√£o sint√©tica a uma c√≥pia do array de features original\n",
    "    para simular uma forma incorreta comum para o tipo de exerc√≠cio especificado.\n",
    "\n",
    "    Args:\n",
    "        original_features (np.ndarray): O array de features (√¢ngulos) da pose original (correta).\n",
    "        exercise_type_str (str): O nome do exerc√≠cio (ex: 'squat', 'deadlift', 'bicep curl').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: O array de features modificado para simular a forma incorreta.\n",
    "    \"\"\"\n",
    "    modified_features = original_features.copy()\n",
    "    \n",
    "    # Magnitude da perturba√ß√£o angular (em graus)\n",
    "    deviation_magnitude = np.random.uniform(15, 35) # Desvio de 15 a 35 graus\n",
    "\n",
    "    if exercise_type_str == 'squat':\n",
    "        # Erro comum: N√£o ir fundo o suficiente (joelhos n√£o dobram o bastante)\n",
    "        # Aumentar o √¢ngulo do joelho para simular falta de profundidade\n",
    "        modified_features[4] = np.clip(modified_features[4] + deviation_magnitude, 0, 180) # Left knee\n",
    "        modified_features[5] = np.clip(modified_features[5] + deviation_magnitude, 0, 180) # Right knee\n",
    "        \n",
    "    elif exercise_type_str == 'deadlift':\n",
    "        # Erro comum: Costas arredondadas (quadril muito baixo ou muito reto)\n",
    "        # Diminuir o √¢ngulo do quadril para simular um tronco menos inclinado para frente ou mais arredondado\n",
    "        modified_features[6] = np.clip(modified_features[6] - deviation_magnitude, 0, 180) # Left hip\n",
    "        modified_features[7] = np.clip(modified_features[7] - deviation_magnitude, 0, 180) # Right hip\n",
    "        \n",
    "    elif exercise_type_str == 'bicep curl':\n",
    "        # Erro comum: Extens√£o incompleta (n√£o esticar totalmente o bra√ßo na parte inferior)\n",
    "        # Manter o √¢ngulo do cotovelo maior do que deveria estar na extens√£o m√°xima\n",
    "        modified_features[0] = np.clip(modified_features[0] - deviation_magnitude, 0, 180) # Left elbow (make it less straight)\n",
    "        modified_features[1] = np.clip(modified_features[1] - deviation_magnitude, 0, 180) # Right elbow (make it less straight)\n",
    "        \n",
    "    # Para outros exerc√≠cios n√£o especificados, o desvio n√£o ser√° aplicado.\n",
    "    return modified_features\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de extra√ß√£o de pose, c√°lculo de √¢ngulos e gera√ß√£o de erros sint√©ticos prontas.\")\n",
    "\n",
    "# 4. Baixar e Pr√©-processar o Dataset de Exerc√≠cios (Com Gera√ß√£o de Erros)\n",
    "\n",
    "# Definir o nome do dataset do Kaggle para exerc√≠cios\n",
    "DATASET_SLUG = \"hasyimabdillah/workoutexercises-images\"\n",
    "DATASET_NAME = \"workout-exercises-images\" # Nome da pasta onde ser√° extra√≠do\n",
    "DATASET_PATH = Path(DATASET_NAME) # Caminho local\n",
    "\n",
    "# Fun√ß√£o para baixar e extrair o dataset do Kaggle\n",
    "def download_exercise_dataset(dataset_slug, target_path):\n",
    "    if not target_path.exists():\n",
    "        print(f\"‚¨áÔ∏è Baixando {dataset_slug} do Kaggle...\")\n",
    "        try:\n",
    "            # Comando Kaggle para baixar e descompactar\n",
    "            # O subprocess.run precisa que o comando 'kaggle' esteja acess√≠vel no PATH do ambiente\n",
    "            subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_slug, \"-p\", str(target_path.parent), \"--unzip\"], check=True)\n",
    "            print(f\"‚úÖ {dataset_slug} baixado e extra√≠do para {target_path.parent}!\")\n",
    "            \n",
    "            # O Kaggle pode baixar para uma pasta com o nome do slug, vamos renomear se necess√°rio\n",
    "            # e garantir que a estrutura seja a esperada.\n",
    "            downloaded_dir = target_path.parent / dataset_slug.split('/')[-1]\n",
    "            if downloaded_dir.exists() and downloaded_dir != target_path:\n",
    "                print(f\"Renomeando '{downloaded_dir}' para '{target_path}'...\")\n",
    "                shutil.move(downloaded_dir, target_path)\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Erro ao baixar {dataset_slug} do Kaggle: {e}\")\n",
    "            print(\"Por favor, verifique se suas credenciais do Kaggle est√£o corretas e se voc√™ tem permiss√£o para baixar o dataset.\")\n",
    "            print(\"Voc√™ pode precisar aceitar as regras do dataset no Kaggle primeiro: https://www.kaggle.com/datasets/hasyimabdillah/workoutexercises-images/code\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro inesperado ao baixar {dataset_slug}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Dataset '{DATASET_NAME}' j√° existe em {target_path}. Pulando download.\")\n",
    "    return True\n",
    "\n",
    "# Baixar o dataset de exerc√≠cios\n",
    "dataset_downloaded = download_exercise_dataset(DATASET_SLUG, DATASET_PATH)\n",
    "\n",
    "if not dataset_downloaded:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel continuar sem o dataset. Encerrando o script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Listar os tipos de exerc√≠cios (subpastas dentro do dataset)\n",
    "exercise_types = [d.name for d in DATASET_PATH.iterdir() if d.is_dir()]\n",
    "print(f\"\\n‚úÖ Tipos de exerc√≠cios encontrados no dataset: {exercise_types}\")\n",
    "\n",
    "# Processamento dos Dados de Exerc√≠cios: Extrair Features e Gerar R√≥tulos de Forma (Correta/Incorreta)\n",
    "\n",
    "# Listas para armazenar as features e os r√≥tulos\n",
    "all_features = []\n",
    "all_exercise_labels = []\n",
    "all_form_labels = [] # 0 para correto, 1 para incorreto\n",
    "\n",
    "# Inicializar o objeto MediaPipe Pose para usar na extra√ß√£o de features\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_detector:\n",
    "    for exercise_type in exercise_types:\n",
    "        exercise_dir = DATASET_PATH / exercise_type\n",
    "        image_files = [f for f in exercise_dir.iterdir() if f.suffix.lower() in ('.jpg', '.jpeg', '.png')]\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è Processando {len(image_files)} imagens para o exerc√≠cio: {exercise_type}\")\n",
    "        \n",
    "        # Amostragem para limitar o tamanho do dataset e acelerar o processamento durante o desenvolvimento.\n",
    "        # Reduzir este n√∫mero acelerar√° o tempo de pr√©-processamento e treinamento.\n",
    "        SAMPLE_PER_EXERCISE = 400 # Processar at√© 400 imagens por tipo de exerc√≠cio para um teste r√°pido\n",
    "        \n",
    "        if len(image_files) > SAMPLE_PER_EXERCISE:\n",
    "            # Use list() para garantir que a amostra seja uma lista de caminhos, n√£o um array numpy\n",
    "            image_files_sampled = np.random.choice(image_files, SAMPLE_PER_EXERCISE, replace=False).tolist()\n",
    "            print(f\"   Amostrando {SAMPLE_PER_EXERCISE} imagens de {len(image_files)} dispon√≠veis para {exercise_type}.\")\n",
    "        else:\n",
    "            image_files_sampled = image_files.tolist() # Se menos que a amostra, usar todas.\n",
    "\n",
    "        # Definir a propor√ß√£o de exemplos \"incorretos\" que queremos gerar sinteticamente\n",
    "        # Isso ajuda a balancear o dataset para as classes \"correta\" e \"incorreta\"\n",
    "        synthetic_incorrect_ratio = 0.4 # Queremos que 40% das amostras processadas sejam sinteticamente incorretas\n",
    "        num_samples_to_make_incorrect = int(len(image_files_sampled) * synthetic_incorrect_ratio)\n",
    "        \n",
    "        # Selecionar aleatoriamente os √≠ndices das imagens onde aplicaremos o erro sint√©tico\n",
    "        # Certifique-se de que n√£o tentamos selecionar mais √≠ndices do que amostras dispon√≠veis\n",
    "        indices_to_make_incorrect = np.random.choice(\n",
    "            len(image_files_sampled),\n",
    "            min(num_samples_to_make_incorrect, len(image_files_sampled)),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, img_file in enumerate(tqdm(image_files_sampled, desc=f\"   Extraindo features de {exercise_type}\")):\n",
    "            original_features = extract_pose_features(str(img_file), pose_detector)\n",
    "            \n",
    "            if original_features is not None:\n",
    "                # Decide if this sample should be made synthetically incorrect\n",
    "                if i in indices_to_make_incorrect:\n",
    "                    # Apply synthetic error and label as incorrect\n",
    "                    features_to_add = synthesize_incorrect_form(original_features, exercise_type)\n",
    "                    form_status = 1 # Incorreto\n",
    "                else:\n",
    "                    # Keep original features and label as correct\n",
    "                    features_to_add = original_features\n",
    "                    form_status = 0 # Correto\n",
    "\n",
    "                all_features.append(features_to_add)\n",
    "                all_exercise_labels.append(exercise_type)\n",
    "                all_form_labels.append(form_status)\n",
    "            else:\n",
    "                pass # Nenhuma pose detectada ou erro de carregamento de imagem\n",
    "\n",
    "\n",
    "# Verificar se h√° dados para processar\n",
    "if not all_features:\n",
    "    print(\"\\n‚ùå Nenhuma feature foi extra√≠da. Verifique seu dataset e a detec√ß√£o de pose.\")\n",
    "    print(\"O script ser√° encerrado pois n√£o h√° dados para treinar o modelo.\")\n",
    "    sys.exit(1) # Encerrar o script\n",
    "\n",
    "# Converter listas para arrays NumPy\n",
    "X = np.array(all_features)\n",
    "y_exercise_raw = np.array(all_exercise_labels)\n",
    "y_form = np.array(all_form_labels)\n",
    "\n",
    "# Codificar r√≥tulos de exerc√≠cio e forma para formato num√©rico (one-hot encoding)\n",
    "# Para exerc√≠cios, precisamos de um LabelEncoder primeiro para mapear strings para inteiros.\n",
    "exercise_encoder = LabelEncoder()\n",
    "y_exercise_encoded = exercise_encoder.fit_transform(y_exercise_raw)\n",
    "y_exercise = to_categorical(y_exercise_encoded)\n",
    "\n",
    "# Para a forma, j√° temos 0/1, ent√£o podemos diretamente para one-hot se necess√°rio.\n",
    "# Como √© bin√°rio (0/1), to_categorical √© √∫til para a fun√ß√£o de perda.\n",
    "y_form = to_categorical(y_form, num_classes=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Total de {len(X)} amostras processadas.\")\n",
    "print(f\"   Shape das features (X): {X.shape}\")\n",
    "print(f\"   Shape dos r√≥tulos de exerc√≠cio (y_exercise): {y_exercise.shape}\")\n",
    "print(f\"   Shape dos r√≥tulos de forma (y_form): {y_form.shape}\")\n",
    "print(f\"   Exerc√≠cios detectados: {exercise_encoder.classes_}\")\n",
    "\n",
    "# Criar diret√≥rio para salvar modelos e scalers\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(exist_ok=True) # Garantir que a pasta models existe\n",
    "\n",
    "# Salvar o LabelEncoder para uso posterior na infer√™ncia\n",
    "joblib.dump(exercise_encoder, MODELS_DIR / 'exercise_encoder.pkl')\n",
    "print(f\"‚úÖ exercise_encoder salvo em {MODELS_DIR / 'exercise_encoder.pkl'}\")\n",
    "\n",
    "# Normalizar as features (importante para redes neurais)\n",
    "# Usaremos MinMaxScaler para escalar as features entre 0 e 1.\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "joblib.dump(feature_scaler, MODELS_DIR / 'feature_scaler.pkl') # Salvar o feature_scaler\n",
    "print(f\"‚úÖ feature_scaler salvo em {MODELS_DIR / 'feature_scaler.pkl'}\")\n",
    "\n",
    "print(\"‚úÖ Pr√©-processamento de dados de exerc√≠cios conclu√≠do e escalers salvos.\")\n",
    "\n",
    "# 5. Constru√ß√£o e Treinamento do Modelo de Detec√ß√£o de Forma\n",
    "\n",
    "# Fun√ß√£o para construir e compilar um modelo Keras (MLP)\n",
    "def build_keras_mlp_model(input_shape, num_exercise_classes):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "    x = Dense(512, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x) # Ajuda na estabilidade do treinamento\n",
    "    x = Dropout(0.4)(x) # Previne overfitting\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Duas sa√≠das para as duas tarefas de classifica√ß√£o\n",
    "    exercise_output = Dense(num_exercise_classes, activation='softmax', name='exercise_output')(x)\n",
    "    form_output = Dense(2, activation='softmax', name='form_output')(x) # 2 classes: correto/incorreto\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=[exercise_output, form_output])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'exercise_output': 'categorical_crossentropy',\n",
    "            'form_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'exercise_output': 0.7, # Maior peso para classificar o exerc√≠cio corretamente\n",
    "            'form_output': 0.3 # Menor peso, mas ainda importante para a forma\n",
    "        },\n",
    "        metrics={\n",
    "            'exercise_output': ['accuracy'],\n",
    "            'form_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Fun√ß√£o para treinar e salvar modelos Scikit-learn (Random Forest, Logistic Regression)\n",
    "# Estes modelos predizem apenas o tipo de exerc√≠cio\n",
    "def train_and_save_sklearn_model(model_class, model_name, X_train, y_exercise_train_flat, X_val, y_exercise_val_flat):\n",
    "    print(f\"‚öôÔ∏è Treinando modelo Scikit-learn: {model_name}\")\n",
    "    # Algumas classes de modelo podem precisar de hiperpar√¢metros espec√≠ficos\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    elif model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    else: # Fallback para outros, embora tenhamos apenas RF e LR por enquanto\n",
    "        model = model_class(random_state=42)\n",
    "        \n",
    "    model.fit(X_train, y_exercise_train_flat)\n",
    "    \n",
    "    # Avaliar no conjunto de valida√ß√£o\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_exercise_val_flat, y_pred_val)\n",
    "    print(f\"   Acur√°cia de Valida√ß√£o para {model_name}: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Constru√ß√£o da string do nome do arquivo separadamente para evitar conflito de aspas\n",
    "    model_filename_suffix = f'best_{model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "    model_filepath = MODELS_DIR / model_filename_suffix\n",
    "\n",
    "    joblib.dump(model, model_filepath) # Usa o Path constru√≠do\n",
    "    print(f\"‚úÖ Modelo '{model_name}' treinado e salvo em {model_filepath}\") # Usa o Path constru√≠do\n",
    "    return model, val_accuracy\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e valida√ß√£o\n",
    "X_train, X_val, y_exercise_train, y_exercise_val, y_form_train, y_form_val = train_test_split(\n",
    "    X_scaled, y_exercise, y_form, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Converter y_exercise_train/val de one-hot para r√≥tulos flat para modelos Scikit-learn\n",
    "y_exercise_train_flat = np.argmax(y_exercise_train, axis=1)\n",
    "y_exercise_val_flat = np.argmax(y_exercise_val, axis=1)\n",
    "\n",
    "num_features = X_scaled.shape[1]\n",
    "num_exercise_classes = y_exercise.shape[1]\n",
    "\n",
    "# Dicion√°rio para armazenar o desempenho de cada modelo\n",
    "model_performance = {}\n",
    "\n",
    "# --- Treinar e Avaliar o Modelo Keras MLP ---\n",
    "mlp_model_path = MODELS_DIR / 'best_mlp_model.h5'\n",
    "if os.path.exists(mlp_model_path):\n",
    "    print(f\"‚ÑπÔ∏è Modelo MLP pr√©-treinado encontrado. Carregando...\")\n",
    "    mlp_model = load_model(mlp_model_path)\n",
    "    print(f\"‚úÖ Modelo MLP carregado.\")\n",
    "else:\n",
    "    print(f\"‚öôÔ∏è Modelo MLP n√£o encontrado. Iniciando constru√ß√£o e treinamento...\")\n",
    "    mlp_model = build_keras_mlp_model(num_features, num_exercise_classes)\n",
    "    \n",
    "    checkpoint_mlp = ModelCheckpoint(\n",
    "        mlp_model_path,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr_mlp = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stop_mlp = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Iniciando treinamento para o modelo MLP...\")\n",
    "    history_mlp = mlp_model.fit(\n",
    "        X_train,\n",
    "        {'exercise_output': y_exercise_train, 'form_output': y_form_train},\n",
    "        epochs=100000,\n",
    "        batch_size=1080,\n",
    "        validation_data=(X_val, {'exercise_output': y_exercise_val, 'form_output': y_form_val}),\n",
    "        callbacks=[checkpoint_mlp, reduce_lr_mlp, early_stop_mlp],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(f\"‚úÖ Modelo MLP treinado e salvo em {mlp_model_path}\")\n",
    "\n",
    "# Avaliar o modelo MLP no conjunto de valida√ß√£o\n",
    "print(f\"Avaliando o modelo MLP no conjunto de valida√ß√£o...\")\n",
    "val_results_mlp = mlp_model.evaluate(X_val, {'exercise_output': y_exercise_val, 'form_output': y_form_val}, verbose=0)\n",
    "val_loss_mlp = val_results_mlp[0]\n",
    "val_exercise_accuracy_mlp = val_results_mlp[3] # √çndice 3 para accuracy da exercise_output\n",
    "val_form_accuracy_mlp = val_results_mlp[4] # √çndice 4 para accuracy da form_output\n",
    "model_performance['MLP Keras (Exerc√≠cio & Forma)'] = {\n",
    "    'val_loss': val_loss_mlp,\n",
    "    'val_exercise_accuracy': val_exercise_accuracy_mlp,\n",
    "    'val_form_accuracy': val_form_accuracy_mlp,\n",
    "    'type': 'keras_multi_output',\n",
    "    'path': mlp_model_path\n",
    "}\n",
    "print(f\"MLP Keras - Perda de Valida√ß√£o: {val_loss_mlp:.4f}, Acur√°cia Exerc√≠cio de Valida√ß√£o: {val_exercise_accuracy_mlp:.4f}, Acur√°cia Forma de Valida√ß√£o: {val_form_accuracy_mlp:.4f}\")\n",
    "\n",
    "\n",
    "# --- Treinar e Avaliar Modelos Scikit-learn ---\n",
    "# Random Forest\n",
    "rf_model, rf_val_accuracy = train_and_save_sklearn_model(RandomForestClassifier, \"Random Forest\", X_train, y_exercise_train_flat, X_val, y_exercise_val_flat)\n",
    "model_performance['Random Forest (Apenas Exerc√≠cio)'] = {\n",
    "    'val_exercise_accuracy': rf_val_accuracy,\n",
    "    'type': 'sklearn_single_output',\n",
    "    'path': MODELS_DIR / f'best_random_forest_model.pkl' # Este j√° estava ok\n",
    "}\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model, lr_val_accuracy = train_and_save_sklearn_model(LogisticRegression, \"Regress√£o Log√≠stica\", X_train, y_exercise_train_flat, X_val, y_exercise_val_flat)\n",
    "model_performance['Regress√£o Log√≠stica (Apenas Exerc√≠cio)'] = {\n",
    "    'val_exercise_accuracy': lr_val_accuracy,\n",
    "    'type': 'sklearn_single_output',\n",
    "    'path': MODELS_DIR / f'best_logistic_regression_model.pkl' # Este j√° estava ok\n",
    "}\n",
    "\n",
    "\n",
    "# --- Selecionar o Modelo Mais Eficiente ---\n",
    "best_model_for_exercise_classification_name = None\n",
    "highest_exercise_accuracy = -1\n",
    "\n",
    "print(\"\\n--- Resultados da Compara√ß√£o de Modelos (Valida√ß√£o) ---\")\n",
    "for name, metrics in model_performance.items():\n",
    "    print(f\"Modelo: {name}\")\n",
    "    if 'val_loss' in metrics:\n",
    "        print(f\"  Perda de Valida√ß√£o: {metrics['val_loss']:.4f}\")\n",
    "    print(f\"  Acur√°cia de Exerc√≠cio de Valida√ß√£o: {metrics['val_exercise_accuracy']:.4f}\")\n",
    "    if 'val_form_accuracy' in metrics:\n",
    "        print(f\"  Acur√°cia de Forma de Valida√ß√£o: {metrics['val_form_accuracy']:.4f}\")\n",
    "    \n",
    "    # Para a compara√ß√£o de qual modelo √© \"mais eficiente\" para o prop√≥sito do aplicativo,\n",
    "    # o modelo Keras MLP √© o √∫nico que oferece detec√ß√£o de forma.\n",
    "    # No entanto, para fins de relat√≥rio de compara√ß√£o de acur√°cia de exerc√≠cio,\n",
    "    # identificamos o de maior acur√°cia de exerc√≠cio.\n",
    "    if metrics['val_exercise_accuracy'] > highest_exercise_accuracy:\n",
    "        highest_exercise_accuracy = metrics['val_exercise_accuracy']\n",
    "        best_model_for_exercise_classification_name = name\n",
    "\n",
    "print(f\"\\nüèÜ Modelo com maior acur√°cia para Classifica√ß√£o de Exerc√≠cio (com base na acur√°cia de valida√ß√£o): '{best_model_for_exercise_classification_name}' com acur√°cia: {highest_exercise_accuracy:.4f}\")\n",
    "\n",
    "# Para a aplica√ß√£o em tempo real, o modelo Keras MLP √© escolhido porque √© o √∫nico\n",
    "# que fornece ambas as detec√ß√µes: tipo de exerc√≠cio E forma.\n",
    "final_model_to_use_name = \"MLP Keras (Exerc√≠cio & Forma)\"\n",
    "final_model_to_use_path = model_performance[final_model_to_use_name]['path']\n",
    "final_model_type = model_performance[final_model_to_use_name]['type']\n",
    "\n",
    "print(f\"\\nEscolha final para aplica√ß√£o em tempo real (devido √† sa√≠da combinada de exerc√≠cio e forma): '{final_model_to_use_name}'\")\n",
    "print(f\"Carregando este modelo para detec√ß√£o em tempo real: {final_model_to_use_path}\")\n",
    "\n",
    "# Carregar o modelo selecionado final para detec√ß√£o em tempo real\n",
    "if final_model_type == 'keras_multi_output':\n",
    "    model = load_model(final_model_to_use_path)\n",
    "    # Garantir que exercise_encoder e feature_scaler estejam carregados se n√£o estiverem na mem√≥ria\n",
    "    if 'exercise_encoder' not in locals() or 'feature_scaler' not in locals():\n",
    "        exercise_encoder = joblib.load(MODELS_DIR / 'exercise_encoder.pkl')\n",
    "        feature_scaler = joblib.load(MODELS_DIR / 'feature_scaler.pkl')\n",
    "else:\n",
    "    # Este caso n√£o deve ser alcan√ßado se o MLP Keras for sempre escolhido para tempo real\n",
    "    # mas inclu√≠do para completude caso a l√≥gica mude.\n",
    "    model = joblib.load(final_model_to_use_path)\n",
    "    if 'exercise_encoder' not in locals() or 'feature_scaler' not in locals():\n",
    "        exercise_encoder = joblib.load(MODELS_DIR / 'exercise_encoder.pkl')\n",
    "        feature_scaler = joblib.load(MODELS_DIR / 'feature_scaler.pkl')\n",
    "    print(\"AVISO: Um modelo n√£o-Keras multi-sa√≠da foi selecionado para tempo real. O feedback de forma pode ser limitado.\")\n",
    "\n",
    "\n",
    "# 6. Detec√ß√£o de Forma de Exerc√≠cios em Tempo Real\n",
    "\n",
    "# Mapeamento para os r√≥tulos de forma\n",
    "form_labels = ['Forma Correta', 'Forma Incorreta']\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detec√ß√£o em tempo real\n",
    "# 'static_image_mode=False' para melhor desempenho em streams de v√≠deo.\n",
    "# O MediaPipe automaticamente usa a CPU neste ambiente.\n",
    "pose_live_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Fun√ß√£o de predi√ß√£o em tempo real\n",
    "def detect_exercise_form(frame, pose_detector, model, feature_scaler, exercise_encoder, form_labels, model_type):\n",
    "    \"\"\"\n",
    "    Processa um frame da webcam para detectar a pose e prever o exerc√≠cio e a forma.\n",
    "    Args:\n",
    "        frame (numpy.ndarray): O frame de v√≠deo da webcam.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado (executando em CPU).\n",
    "        model (tensorflow.keras.Model): O modelo de ML treinado (infer√™ncia em CPU).\n",
    "        feature_scaler (MinMaxScaler): O scaler usado para normalizar as features.\n",
    "        exercise_encoder (LabelEncoder): O encoder usado para os r√≥tulos de exerc√≠cio.\n",
    "        form_labels (list): Lista de r√≥tulos para a forma (e.g., ['Correto', 'Incorreto']).\n",
    "        model_type (str): Tipo do modelo (e.g., 'keras_multi_output', 'sklearn_single_output').\n",
    "    Returns:\n",
    "        numpy.ndarray: O frame com as informa√ß√µes de pose e feedback.\n",
    "    \"\"\"\n",
    "    # Converter o frame para RGB para o MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Melhorar o desempenho (opcional): marcar a imagem como n√£o grav√°vel para passar por refer√™ncia.\n",
    "    image_rgb.flags.writeable = False\n",
    "    \n",
    "    # Processar o frame para estimativa de pose (executa na CPU)\n",
    "    results = pose_detector.process(image_rgb)\n",
    "    \n",
    "    # Marcar a imagem como grav√°vel novamente para desenhar as anota√ß√µes\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # Voltar para BGR para o OpenCV\n",
    "    \n",
    "    feedback_text = \"Nenhuma pose detectada.\"\n",
    "    text_color = (255, 255, 255) # Cor padr√£o branca para feedback\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Desenhar as landmarks da pose no frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_bgr,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Extrair features da pose detectada no frame atual\n",
    "        # Replicamos a l√≥gica de extract_pose_features para o frame atual\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        try:\n",
    "            # Coletar as mesmas features que foram usadas no treinamento\n",
    "            live_features = np.array([\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                )\n",
    "            ])\n",
    "            \n",
    "            # Normalizar as features usando o scaler treinado\n",
    "            live_features_scaled = feature_scaler.transform(live_features.reshape(1, -1))\n",
    "            \n",
    "            if model_type == 'keras_multi_output':\n",
    "                predictions = model.predict(live_features_scaled, verbose=0)\n",
    "                exercise_probs = predictions[0][0]\n",
    "                form_probs = predictions[1][0]\n",
    "                \n",
    "                predicted_exercise_idx = np.argmax(exercise_probs)\n",
    "                predicted_exercise = exercise_encoder.inverse_transform([predicted_exercise_idx])[0]\n",
    "                \n",
    "                predicted_form_idx = np.argmax(form_probs)\n",
    "                predicted_form = form_labels[predicted_form_idx]\n",
    "                \n",
    "                feedback_text = f\"Exercicio: {predicted_exercise} ({exercise_probs[predicted_exercise_idx]*100:.1f}%) | Forma: {predicted_form} ({form_probs[predicted_form_idx]*100:.1f}%)\"\n",
    "                \n",
    "                if predicted_form_idx == 1: # Se a forma for incorreta\n",
    "                    text_color = (0, 0, 255) # Vermelho\n",
    "                    feedback_text += \" -> Ajuste sua postura!\"\n",
    "                else:\n",
    "                    text_color = (0, 255, 0) # Verde para correto\n",
    "            else: # Modelos Scikit-learn (ou qualquer outro modelo de sa√≠da √∫nica para exerc√≠cio)\n",
    "                # Estes modelos predizem apenas o tipo de exerc√≠cio.\n",
    "                # Assumindo que os modelos sklearn foram selecionados por sua maior acur√°cia de exerc√≠cio.\n",
    "                # Para a 'forma', vamos assumir 'Correta' por padr√£o, j√° que eles n√£o a preveem explicitamente.\n",
    "                exercise_pred_id = model.predict(live_features_scaled)[0]\n",
    "                exercise_probs_all = model.predict_proba(live_features_scaled)[0] # Obter probabilidades para todas as classes\n",
    "                \n",
    "                predicted_exercise = exercise_encoder.inverse_transform([exercise_pred_id])[0]\n",
    "                confidence = exercise_probs_all[exercise_pred_id]\n",
    "                \n",
    "                # Forma padr√£o para correta para modelos de sa√≠da √∫nica, pois eles n√£o preveem a forma explicitamente\n",
    "                predicted_form = \"Forma Correta\" \n",
    "                predicted_form_idx = 0 # Padr√£o para √≠ndice de forma correta\n",
    "                form_probs = np.array([1.0, 0.0]) # Assumir 100% de confian√ßa na forma correta para fins de exibi√ß√£o\n",
    "                \n",
    "                feedback_text = f\"Exercicio: {predicted_exercise} ({confidence*100:.1f}%) | Forma: {predicted_form}\"\n",
    "                text_color = (0, 255, 0) # Sempre verde se a forma for assumida como correta\n",
    "\n",
    "        except Exception as e:\n",
    "            feedback_text = f\"Erro na predi√ß√£o: {e}\"\n",
    "            text_color = (0, 165, 255) # Laranja para erro\n",
    "            print(f\"Erro na detec√ß√£o: {e}\")\n",
    "\n",
    "    # Exibir o feedback no frame\n",
    "    cv2.putText(image_bgr, feedback_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image_bgr\n",
    "\n",
    "# Loop da Webcam para Detec√ß√£o em Tempo Real\n",
    "\n",
    "# Inicializar a captura de v√≠deo da webcam (0 √© geralmente a c√¢mera padr√£o)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå N√£o foi poss√≠vel acessar a webcam. Verifique se ela est√° conectada e n√£o est√° em uso.\")\n",
    "else:\n",
    "    # Definir resolu√ß√£o da webcam para melhor qualidade\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    window_name = 'Detecao de Forma de Exercicios (CPU) - Pressione Q para Sair'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    print(\"\\n‚úÖ Webcam ativada. Pressione 'Q' para sair da janela de visualiza√ß√£o.\")\n",
    "    # Exibir a hora atual conforme as instru√ß√µes\n",
    "    current_utc_time = time.gmtime()\n",
    "    current_brasilia_time = time.localtime(time.time() - 3 * 3600) # UTC-3\n",
    "    print(f\"UTC: {time.strftime('%d/%m/%Y %H:%M:%S', current_utc_time)} (UTC)\")\n",
    "    print(f\"Bras√≠lia: {time.strftime('%d/%m/%Y %H:%M:%S', current_brasilia_time)} (UTC-3)\")\n",
    "    print(\"\\n‚ö†Ô∏è A performance pode ser limitada pela sua CPU. Para melhor experi√™ncia, considere GPUs dedicadas.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"‚ùå Falha ao ler frame da webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Espelhar o frame para que a visualiza√ß√£o seja mais intuitiva (como um espelho)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Chamar a fun√ß√£o de detec√ß√£o e obter o frame com feedback\n",
    "            output_frame = detect_exercise_form(frame, pose_live_detector, model, feature_scaler, exercise_encoder, form_labels, final_model_type)\n",
    "            \n",
    "            # Mostrar o frame na janela\n",
    "            cv2.imshow(window_name, output_frame)\n",
    "            \n",
    "            # Verificar se a janela foi fechada pelo usu√°rio\n",
    "            if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "                \n",
    "            # Sair do loop se a tecla 'q' for pressionada\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        # Liberar os recursos da webcam e fechar todas as janelas do OpenCV\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"‚úÖ Programa finalizado. Recursos liberados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39cb7b-1eb5-46e2-b434-d7a18578882e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
