{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde01d2f-77b1-44ab-8cf0-636e042d870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Nenhuma GPU NVIDIA detectada ou configurada. Todas as operações de Machine Learning utilizarão a CPU.\n",
      "✅ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\n",
      "✅ Funções de extração de pose, cálculo de ângulos e geração de erros sintéticos prontas.\n",
      "ℹ️ Dataset 'workout-exercises-images' já existe em workout-exercises-images. Pulando download.\n",
      "\n",
      "✅ Tipos de exercícios encontrados no dataset: ['barbell biceps curl', 'bench press', 'chest fly machine', 'deadlift', 'decline bench press', 'hammer curl', 'hip thrust', 'incline bench press', 'lat pulldown', 'lateral raises', 'leg extension', 'leg raises', 'plank', 'pull up', 'push up', 'romanian deadlift', 'russian twist', 'shoulder press', 'squat', 't bar row', 'tricep dips', 'tricep pushdown']\n",
      "\n",
      "⚙️ Processando 705 imagens para o exercício: barbell biceps curl\n",
      "   Amostrando 400 imagens de 705 disponíveis para barbell biceps curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de barbell biceps curl: 100%|██████████████████████████████████| 400/400 [00:21<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 625 imagens para o exercício: bench press\n",
      "   Amostrando 400 imagens de 625 disponíveis para bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de bench press: 100%|██████████████████████████████████████████| 400/400 [00:19<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 527 imagens para o exercício: chest fly machine\n",
      "   Amostrando 400 imagens de 527 disponíveis para chest fly machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de chest fly machine: 100%|████████████████████████████████████| 400/400 [00:21<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 530 imagens para o exercício: deadlift\n",
      "   Amostrando 400 imagens de 530 disponíveis para deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de deadlift: 100%|█████████████████████████████████████████████| 400/400 [00:21<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 514 imagens para o exercício: decline bench press\n",
      "   Amostrando 400 imagens de 514 disponíveis para decline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de decline bench press: 100%|██████████████████████████████████| 400/400 [00:17<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 546 imagens para o exercício: hammer curl\n",
      "   Amostrando 400 imagens de 546 disponíveis para hammer curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hammer curl: 100%|██████████████████████████████████████████| 400/400 [00:21<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 557 imagens para o exercício: hip thrust\n",
      "   Amostrando 400 imagens de 557 disponíveis para hip thrust.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hip thrust: 100%|███████████████████████████████████████████| 400/400 [00:21<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 729 imagens para o exercício: incline bench press\n",
      "   Amostrando 400 imagens de 729 disponíveis para incline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de incline bench press: 100%|██████████████████████████████████| 400/400 [00:20<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 646 imagens para o exercício: lat pulldown\n",
      "   Amostrando 400 imagens de 646 disponíveis para lat pulldown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lat pulldown: 100%|█████████████████████████████████████████| 400/400 [00:20<00:00, 19.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 843 imagens para o exercício: lateral raises\n",
      "   Amostrando 400 imagens de 843 disponíveis para lateral raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lateral raises: 100%|███████████████████████████████████████| 400/400 [00:21<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 586 imagens para o exercício: leg extension\n",
      "   Amostrando 400 imagens de 586 disponíveis para leg extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg extension: 100%|████████████████████████████████████████| 400/400 [00:20<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 514 imagens para o exercício: leg raises\n",
      "   Amostrando 400 imagens de 514 disponíveis para leg raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg raises: 100%|███████████████████████████████████████████| 400/400 [00:21<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 993 imagens para o exercício: plank\n",
      "   Amostrando 400 imagens de 993 disponíveis para plank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de plank: 100%|████████████████████████████████████████████████| 400/400 [00:21<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 615 imagens para o exercício: pull up\n",
      "   Amostrando 400 imagens de 615 disponíveis para pull up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de pull up: 100%|██████████████████████████████████████████████| 400/400 [00:21<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 601 imagens para o exercício: push up\n",
      "   Amostrando 400 imagens de 601 disponíveis para push up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de push up: 100%|██████████████████████████████████████████████| 400/400 [00:21<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 555 imagens para o exercício: romanian deadlift\n",
      "   Amostrando 400 imagens de 555 disponíveis para romanian deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de romanian deadlift: 100%|████████████████████████████████████| 400/400 [00:20<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 522 imagens para o exercício: russian twist\n",
      "   Amostrando 400 imagens de 522 disponíveis para russian twist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de russian twist: 100%|████████████████████████████████████████| 400/400 [00:21<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 512 imagens para o exercício: shoulder press\n",
      "   Amostrando 400 imagens de 512 disponíveis para shoulder press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de shoulder press: 100%|███████████████████████████████████████| 400/400 [00:21<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 742 imagens para o exercício: squat\n",
      "   Amostrando 400 imagens de 742 disponíveis para squat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de squat: 100%|████████████████████████████████████████████████| 400/400 [00:21<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 668 imagens para o exercício: t bar row\n",
      "   Amostrando 400 imagens de 668 disponíveis para t bar row.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de t bar row: 100%|████████████████████████████████████████████| 400/400 [00:21<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 698 imagens para o exercício: tricep dips\n",
      "   Amostrando 400 imagens de 698 disponíveis para tricep dips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep dips: 100%|██████████████████████████████████████████| 400/400 [00:21<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 625 imagens para o exercício: tricep pushdown\n",
      "   Amostrando 400 imagens de 625 disponíveis para tricep pushdown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep pushdown: 100%|██████████████████████████████████████| 400/400 [00:21<00:00, 18.52it/s]\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total de 7852 amostras processadas.\n",
      "   Shape das features (X): (7852, 8)\n",
      "   Shape dos rótulos de exercício (y_exercise): (7852, 22)\n",
      "   Shape dos rótulos de forma (y_form): (7852, 2)\n",
      "   Exercícios detectados: ['barbell biceps curl' 'bench press' 'chest fly machine' 'deadlift'\n",
      " 'decline bench press' 'hammer curl' 'hip thrust' 'incline bench press'\n",
      " 'lat pulldown' 'lateral raises' 'leg extension' 'leg raises' 'plank'\n",
      " 'pull up' 'push up' 'romanian deadlift' 'russian twist' 'shoulder press'\n",
      " 'squat' 't bar row' 'tricep dips' 'tricep pushdown']\n",
      "✅ exercise_encoder salvo em models\\exercise_encoder.pkl\n",
      "✅ feature_scaler salvo em models\\feature_scaler.pkl\n",
      "✅ Pré-processamento de dados de exercícios concluído e escalers salvos.\n",
      "ℹ️ Modelo MLP pré-treinado encontrado. Carregando...\n",
      "✅ Modelo MLP carregado.\n",
      "Avaliando o modelo MLP no conjunto de validação...\n",
      "MLP Keras - Perda de Validação: 1.1351, Acurácia Exercício de Validação: 0.5977, Acurácia Forma de Validação: 0.6155\n",
      "⚙️ Treinando modelo Scikit-learn: Random Forest\n",
      "   Acurácia de Validação para Random Forest: 0.6595\n",
      "✅ Modelo 'Random Forest' treinado e salvo em models\\best_random_forest_model.pkl\n",
      "⚙️ Treinando modelo Scikit-learn: Regressão Logística\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Acurácia de Validação para Regressão Logística: 0.3393\n",
      "✅ Modelo 'Regressão Logística' treinado e salvo em models\\best_regressão_logística_model.pkl\n",
      "\n",
      "--- Resultados da Comparação de Modelos (Validação) ---\n",
      "Modelo: MLP Keras (Exercício & Forma)\n",
      "  Perda de Validação: 1.1351\n",
      "  Acurácia de Exercício de Validação: 0.5977\n",
      "  Acurácia de Forma de Validação: 0.6155\n",
      "Modelo: Random Forest (Apenas Exercício)\n",
      "  Acurácia de Exercício de Validação: 0.6595\n",
      "Modelo: Regressão Logística (Apenas Exercício)\n",
      "  Acurácia de Exercício de Validação: 0.3393\n",
      "\n",
      "🏆 Modelo com maior acurácia para Classificação de Exercício (com base na acurácia de validação): 'Random Forest (Apenas Exercício)' com acurácia: 0.6595\n",
      "\n",
      "Escolha final para aplicação em tempo real (devido à saída combinada de exercício e forma): 'MLP Keras (Exercício & Forma)'\n",
      "Carregando este modelo para detecção em tempo real: models\\best_mlp_model.h5\n",
      "\n",
      "✅ Webcam ativada. Pressione 'Q' para sair da janela de visualização.\n",
      "UTC: 09/07/2025 03:05:03 (UTC)\n",
      "Brasília: 08/07/2025 21:05:03 (UTC-3)\n",
      "\n",
      "⚠️ A performance pode ser limitada pela sua CPU. Para melhor experiência, considere GPUs dedicadas.\n",
      "✅ Programa finalizado. Recursos liberados.\n"
     ]
    }
   ],
   "source": [
    "# gym_exercise_corrector.py\n",
    "\n",
    "# 1. Instalação das Dependências\n",
    "# Garantir que todas as bibliotecas necessárias estejam instaladas.\n",
    "# Adicionamos 'mediapipe' que é essencial para a estimativa de pose.\n",
    "# O pacote 'tensorflow' por padrão instala a versão otimizada para CPU.\n",
    "\n",
    "# Removido 'tensorflow-gpu' e 'gdown' já que estamos em CPU e o dataset é baixado com kaggle cli\n",
    "# A linha abaixo deve ser executada uma vez no terminal ou na célula de notebook para instalar/atualizar\n",
    "# !pip install tensorflow numpy pandas matplotlib requests scikit-learn imbalanced-learn tqdm kaggle mediapipe opencv-python --upgrade\n",
    "# !pip install --upgrade h5py Keras\n",
    "\n",
    "# 2. Configurações Iniciais e Verificação de Hardware\n",
    "# Importações essenciais para o projeto.\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import sys # Importado para sys.exit()\n",
    "\n",
    "# Importar MediaPipe para estimativa de pose\n",
    "import mediapipe as mp\n",
    "\n",
    "# Importar bibliotecas para modelos Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib # Para salvar/carregar scalers e modelos Scikit-learn\n",
    "\n",
    "# Configuração de CPU\n",
    "# Em um ambiente com apenas CPU, o TensorFlow irá automaticamente utilizar o processador.\n",
    "print(\"ℹ️ Nenhuma GPU NVIDIA detectada ou configurada. Todas as operações de Machine Learning utilizarão a CPU.\")\n",
    "\n",
    "# Configurar credenciais do Kaggle\n",
    "# Isso é crucial para baixar o dataset de exercícios automaticamente.\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Você pode obter isso no seu perfil do Kaggle, na seção \"API\".\n",
    "kaggle_creds = {\n",
    "    \"username\": \"SEU_USERNAME_KAGGLE\", # Substitua pelo seu username do Kaggle\n",
    "    \"key\": \"SUA_CHAVE_KAGGLE\" # Substitua pela sua chave API do Kaggle\n",
    "}\n",
    "\n",
    "# Salvar credenciais no arquivo kaggle.json\n",
    "with open(kaggle_dir / 'kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "\n",
    "# Adicionar ao PATH para que o comando 'kaggle' possa ser executado\n",
    "# Isso é importante para que o subprocess.run possa encontrar o comando kaggle.\n",
    "os.environ['PATH'] += os.pathsep + str(Path.home() / '.local' / 'bin')\n",
    "\n",
    "print(\"✅ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\")\n",
    "\n",
    "\n",
    "# 3. Definição de Funções Auxiliares para Extração de Features (MediaPipe) e Geração de Erros\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detecção de landmarks.\n",
    "# Static_image_mode=False é para processamento de vídeo, True para imagens estáticas.\n",
    "# Min_detection_confidence e min_tracking_confidence são limiares de confiança.\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles # Corrigido aqui, estava mp.solutions.drawing_utils novamente\n",
    "\n",
    "# Função para calcular o ângulo entre três pontos (landmarks)\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calcula o ângulo em graus entre três pontos (landmarks).\n",
    "    O ponto 'b' é o vértice do ângulo.\n",
    "    Args:\n",
    "        a (list/tuple): Coordenadas (x, y) ou (x, y, z) do primeiro ponto.\n",
    "        b (list/tuple): Coordenadas (x, y) ou (x, y, z) do ponto central (vértice).\n",
    "        c (list/tuple): Coordenadas (x, y) ou (x, y, z) do terceiro ponto.\n",
    "    Returns:\n",
    "        float: O ângulo em graus.\n",
    "    \"\"\"\n",
    "    a = np.array(a) # Primeiro ponto\n",
    "    b = np.array(b) # Ponto do vértice\n",
    "    c = np.array(c) # Terceiro ponto\n",
    "\n",
    "    # Vetores formados pelos pontos\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # Produto escalar e normas dos vetores\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    # Garantir que o valor esteja dentro do domínio de arccos para evitar erros de floating point\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Função para extrair features de pose de uma imagem (ângulos e distâncias)\n",
    "def extract_pose_features(image_path, pose_detector):\n",
    "    \"\"\"\n",
    "    Extrai um vetor de features de pose (ângulos chave do corpo) de uma imagem.\n",
    "    Args:\n",
    "        image_path (str): Caminho para a imagem.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado.\n",
    "    Returns:\n",
    "        numpy.ndarray or None: Um vetor numpy de features (ângulos), ou None se nenhuma pose for detectada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lendo a imagem\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # print(f\"⚠️ Não foi possível carregar a imagem: {image_path}\") # Comentado para evitar poluir o log\n",
    "            return None\n",
    "\n",
    "        # Convertendo a imagem de BGR para RGB (MediaPipe espera RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Processando a imagem para estimativa de pose\n",
    "        results = pose_detector.process(image_rgb)\n",
    "\n",
    "        # Se houver landmarks de pose detectadas\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Mapeamento das landmarks do MediaPipe para fácil acesso\n",
    "            # Para o `mp_pose.PoseLandmark` o `.value` é necessário para acessar o índice correto\n",
    "            \n",
    "            # Ângulos dos cotovelos (ombro, cotovelo, punho)\n",
    "            left_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            )\n",
    "            right_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            )\n",
    "\n",
    "            # Ângulos dos ombros (quadril, ombro, cotovelo)\n",
    "            left_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            )\n",
    "            right_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            )\n",
    "            \n",
    "            # Ângulos dos joelhos (quadril, joelho, tornozelo)\n",
    "            left_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            )\n",
    "            right_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            )\n",
    "\n",
    "            # Ângulos dos quadris (ombro, quadril, joelho) - essencial para postura de tronco\n",
    "            left_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "            )\n",
    "            right_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            )\n",
    "            \n",
    "            # Coletar todas as features em um vetor\n",
    "            features = [\n",
    "                left_elbow_angle, right_elbow_angle,\n",
    "                left_shoulder_angle, right_shoulder_angle,\n",
    "                left_knee_angle, right_knee_angle,\n",
    "                left_hip_angle, right_hip_angle\n",
    "            ]\n",
    "            \n",
    "            return np.array(features)\n",
    "        else:\n",
    "            return None # Nenhuma pose detectada\n",
    "    except Exception as e:\n",
    "        # print(f\"⚠️ Erro ao extrair features de {image_path}: {e}\") # Comentado para evitar poluir o log\n",
    "        return None\n",
    "\n",
    "# Nova função para gerar formas incorretas sinteticamente\n",
    "def synthesize_incorrect_form(original_features, exercise_type_str):\n",
    "    \"\"\"\n",
    "    Aplica uma perturbação sintética a uma cópia do array de features original\n",
    "    para simular uma forma incorreta comum para o tipo de exercício especificado.\n",
    "\n",
    "    Args:\n",
    "        original_features (np.ndarray): O array de features (ângulos) da pose original (correta).\n",
    "        exercise_type_str (str): O nome do exercício (ex: 'squat', 'deadlift', 'bicep curl').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: O array de features modificado para simular a forma incorreta.\n",
    "    \"\"\"\n",
    "    modified_features = original_features.copy()\n",
    "    \n",
    "    # Magnitude da perturbação angular (em graus)\n",
    "    deviation_magnitude = np.random.uniform(15, 35) # Desvio de 15 a 35 graus\n",
    "\n",
    "    if exercise_type_str == 'squat':\n",
    "        # Erro comum: Não ir fundo o suficiente (joelhos não dobram o bastante)\n",
    "        # Aumentar o ângulo do joelho para simular falta de profundidade\n",
    "        modified_features[4] = np.clip(modified_features[4] + deviation_magnitude, 0, 180) # Left knee\n",
    "        modified_features[5] = np.clip(modified_features[5] + deviation_magnitude, 0, 180) # Right knee\n",
    "        \n",
    "    elif exercise_type_str == 'deadlift':\n",
    "        # Erro comum: Costas arredondadas (quadril muito baixo ou muito reto)\n",
    "        # Diminuir o ângulo do quadril para simular um tronco menos inclinado para frente ou mais arredondado\n",
    "        modified_features[6] = np.clip(modified_features[6] - deviation_magnitude, 0, 180) # Left hip\n",
    "        modified_features[7] = np.clip(modified_features[7] - deviation_magnitude, 0, 180) # Right hip\n",
    "        \n",
    "    elif exercise_type_str == 'bicep curl':\n",
    "        # Erro comum: Extensão incompleta (não esticar totalmente o braço na parte inferior)\n",
    "        # Manter o ângulo do cotovelo maior do que deveria estar na extensão máxima\n",
    "        modified_features[0] = np.clip(modified_features[0] - deviation_magnitude, 0, 180) # Left elbow (make it less straight)\n",
    "        modified_features[1] = np.clip(modified_features[1] - deviation_magnitude, 0, 180) # Right elbow (make it less straight)\n",
    "        \n",
    "    # Para outros exercícios não especificados, o desvio não será aplicado.\n",
    "    return modified_features\n",
    "\n",
    "print(\"✅ Funções de extração de pose, cálculo de ângulos e geração de erros sintéticos prontas.\")\n",
    "\n",
    "# 4. Baixar e Pré-processar o Dataset de Exercícios (Com Geração de Erros)\n",
    "\n",
    "# Definir o nome do dataset do Kaggle para exercícios\n",
    "DATASET_SLUG = \"hasyimabdillah/workoutexercises-images\"\n",
    "DATASET_NAME = \"workout-exercises-images\" # Nome da pasta onde será extraído\n",
    "DATASET_PATH = Path(DATASET_NAME) # Caminho local\n",
    "\n",
    "# Função para baixar e extrair o dataset do Kaggle\n",
    "def download_exercise_dataset(dataset_slug, target_path):\n",
    "    if not target_path.exists():\n",
    "        print(f\"⬇️ Baixando {dataset_slug} do Kaggle...\")\n",
    "        try:\n",
    "            # Comando Kaggle para baixar e descompactar\n",
    "            # O subprocess.run precisa que o comando 'kaggle' esteja acessível no PATH do ambiente\n",
    "            subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_slug, \"-p\", str(target_path.parent), \"--unzip\"], check=True)\n",
    "            print(f\"✅ {dataset_slug} baixado e extraído para {target_path.parent}!\")\n",
    "            \n",
    "            # O Kaggle pode baixar para uma pasta com o nome do slug, vamos renomear se necessário\n",
    "            # e garantir que a estrutura seja a esperada.\n",
    "            downloaded_dir = target_path.parent / dataset_slug.split('/')[-1]\n",
    "            if downloaded_dir.exists() and downloaded_dir != target_path:\n",
    "                print(f\"Renomeando '{downloaded_dir}' para '{target_path}'...\")\n",
    "                shutil.move(downloaded_dir, target_path)\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Erro ao baixar {dataset_slug} do Kaggle: {e}\")\n",
    "            print(\"Por favor, verifique se suas credenciais do Kaggle estão corretas e se você tem permissão para baixar o dataset.\")\n",
    "            print(\"Você pode precisar aceitar as regras do dataset no Kaggle primeiro: https://www.kaggle.com/datasets/hasyimabdillah/workoutexercises-images/code\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro inesperado ao baixar {dataset_slug}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"ℹ️ Dataset '{DATASET_NAME}' já existe em {target_path}. Pulando download.\")\n",
    "    return True\n",
    "\n",
    "# Baixar o dataset de exercícios\n",
    "dataset_downloaded = download_exercise_dataset(DATASET_SLUG, DATASET_PATH)\n",
    "\n",
    "if not dataset_downloaded:\n",
    "    print(\"❌ Não foi possível continuar sem o dataset. Encerrando o script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Listar os tipos de exercícios (subpastas dentro do dataset)\n",
    "exercise_types = [d.name for d in DATASET_PATH.iterdir() if d.is_dir()]\n",
    "print(f\"\\n✅ Tipos de exercícios encontrados no dataset: {exercise_types}\")\n",
    "\n",
    "# Processamento dos Dados de Exercícios: Extrair Features e Gerar Rótulos de Forma (Correta/Incorreta)\n",
    "\n",
    "# Listas para armazenar as features e os rótulos\n",
    "all_features = []\n",
    "all_exercise_labels = []\n",
    "all_form_labels = [] # 0 para correto, 1 para incorreto\n",
    "\n",
    "# Inicializar o objeto MediaPipe Pose para usar na extração de features\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_detector:\n",
    "    for exercise_type in exercise_types:\n",
    "        exercise_dir = DATASET_PATH / exercise_type\n",
    "        image_files = [f for f in exercise_dir.iterdir() if f.suffix.lower() in ('.jpg', '.jpeg', '.png')]\n",
    "        \n",
    "        print(f\"\\n⚙️ Processando {len(image_files)} imagens para o exercício: {exercise_type}\")\n",
    "        \n",
    "        # Amostragem para limitar o tamanho do dataset e acelerar o processamento durante o desenvolvimento.\n",
    "        # Reduzir este número acelerará o tempo de pré-processamento e treinamento.\n",
    "        SAMPLE_PER_EXERCISE = 400 # Processar até 400 imagens por tipo de exercício para um teste rápido\n",
    "        \n",
    "        if len(image_files) > SAMPLE_PER_EXERCISE:\n",
    "            # Use list() para garantir que a amostra seja uma lista de caminhos, não um array numpy\n",
    "            image_files_sampled = np.random.choice(image_files, SAMPLE_PER_EXERCISE, replace=False).tolist()\n",
    "            print(f\"   Amostrando {SAMPLE_PER_EXERCISE} imagens de {len(image_files)} disponíveis para {exercise_type}.\")\n",
    "        else:\n",
    "            image_files_sampled = image_files.tolist() # Se menos que a amostra, usar todas.\n",
    "\n",
    "        # Definir a proporção de exemplos \"incorretos\" que queremos gerar sinteticamente\n",
    "        # Isso ajuda a balancear o dataset para as classes \"correta\" e \"incorreta\"\n",
    "        synthetic_incorrect_ratio = 0.4 # Queremos que 40% das amostras processadas sejam sinteticamente incorretas\n",
    "        num_samples_to_make_incorrect = int(len(image_files_sampled) * synthetic_incorrect_ratio)\n",
    "        \n",
    "        # Selecionar aleatoriamente os índices das imagens onde aplicaremos o erro sintético\n",
    "        # Certifique-se de que não tentamos selecionar mais índices do que amostras disponíveis\n",
    "        indices_to_make_incorrect = np.random.choice(\n",
    "            len(image_files_sampled),\n",
    "            min(num_samples_to_make_incorrect, len(image_files_sampled)),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, img_file in enumerate(tqdm(image_files_sampled, desc=f\"   Extraindo features de {exercise_type}\")):\n",
    "            original_features = extract_pose_features(str(img_file), pose_detector)\n",
    "            \n",
    "            if original_features is not None:\n",
    "                # Decide if this sample should be made synthetically incorrect\n",
    "                if i in indices_to_make_incorrect:\n",
    "                    # Apply synthetic error and label as incorrect\n",
    "                    features_to_add = synthesize_incorrect_form(original_features, exercise_type)\n",
    "                    form_status = 1 # Incorreto\n",
    "                else:\n",
    "                    # Keep original features and label as correct\n",
    "                    features_to_add = original_features\n",
    "                    form_status = 0 # Correto\n",
    "\n",
    "                all_features.append(features_to_add)\n",
    "                all_exercise_labels.append(exercise_type)\n",
    "                all_form_labels.append(form_status)\n",
    "            else:\n",
    "                pass # Nenhuma pose detectada ou erro de carregamento de imagem\n",
    "\n",
    "\n",
    "# Verificar se há dados para processar\n",
    "if not all_features:\n",
    "    print(\"\\n❌ Nenhuma feature foi extraída. Verifique seu dataset e a detecção de pose.\")\n",
    "    print(\"O script será encerrado pois não há dados para treinar o modelo.\")\n",
    "    sys.exit(1) # Encerrar o script\n",
    "\n",
    "# Converter listas para arrays NumPy\n",
    "X = np.array(all_features)\n",
    "y_exercise_raw = np.array(all_exercise_labels)\n",
    "y_form = np.array(all_form_labels)\n",
    "\n",
    "# Codificar rótulos de exercício e forma para formato numérico (one-hot encoding)\n",
    "# Para exercícios, precisamos de um LabelEncoder primeiro para mapear strings para inteiros.\n",
    "exercise_encoder = LabelEncoder()\n",
    "y_exercise_encoded = exercise_encoder.fit_transform(y_exercise_raw)\n",
    "y_exercise = to_categorical(y_exercise_encoded)\n",
    "\n",
    "# Para a forma, já temos 0/1, então podemos diretamente para one-hot se necessário.\n",
    "# Como é binário (0/1), to_categorical é útil para a função de perda.\n",
    "y_form = to_categorical(y_form, num_classes=2)\n",
    "\n",
    "print(f\"\\n✅ Total de {len(X)} amostras processadas.\")\n",
    "print(f\"   Shape das features (X): {X.shape}\")\n",
    "print(f\"   Shape dos rótulos de exercício (y_exercise): {y_exercise.shape}\")\n",
    "print(f\"   Shape dos rótulos de forma (y_form): {y_form.shape}\")\n",
    "print(f\"   Exercícios detectados: {exercise_encoder.classes_}\")\n",
    "\n",
    "# Criar diretório para salvar modelos e scalers\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(exist_ok=True) # Garantir que a pasta models existe\n",
    "\n",
    "# Salvar o LabelEncoder para uso posterior na inferência\n",
    "joblib.dump(exercise_encoder, MODELS_DIR / 'exercise_encoder.pkl')\n",
    "print(f\"✅ exercise_encoder salvo em {MODELS_DIR / 'exercise_encoder.pkl'}\")\n",
    "\n",
    "# Normalizar as features (importante para redes neurais)\n",
    "# Usaremos MinMaxScaler para escalar as features entre 0 e 1.\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "joblib.dump(feature_scaler, MODELS_DIR / 'feature_scaler.pkl') # Salvar o feature_scaler\n",
    "print(f\"✅ feature_scaler salvo em {MODELS_DIR / 'feature_scaler.pkl'}\")\n",
    "\n",
    "print(\"✅ Pré-processamento de dados de exercícios concluído e escalers salvos.\")\n",
    "\n",
    "# 5. Construção e Treinamento do Modelo de Detecção de Forma\n",
    "\n",
    "# Função para construir e compilar um modelo Keras (MLP)\n",
    "def build_keras_mlp_model(input_shape, num_exercise_classes):\n",
    "    input_layer = Input(shape=(input_shape,))\n",
    "    x = Dense(512, activation='relu')(input_layer)\n",
    "    x = BatchNormalization()(x) # Ajuda na estabilidade do treinamento\n",
    "    x = Dropout(0.4)(x) # Previne overfitting\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Duas saídas para as duas tarefas de classificação\n",
    "    exercise_output = Dense(num_exercise_classes, activation='softmax', name='exercise_output')(x)\n",
    "    form_output = Dense(2, activation='softmax', name='form_output')(x) # 2 classes: correto/incorreto\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=[exercise_output, form_output])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'exercise_output': 'categorical_crossentropy',\n",
    "            'form_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'exercise_output': 0.7, # Maior peso para classificar o exercício corretamente\n",
    "            'form_output': 0.3 # Menor peso, mas ainda importante para a forma\n",
    "        },\n",
    "        metrics={\n",
    "            'exercise_output': ['accuracy'],\n",
    "            'form_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Função para treinar e salvar modelos Scikit-learn (Random Forest, Logistic Regression)\n",
    "# Estes modelos predizem apenas o tipo de exercício\n",
    "def train_and_save_sklearn_model(model_class, model_name, X_train, y_exercise_train_flat, X_val, y_exercise_val_flat):\n",
    "    print(f\"⚙️ Treinando modelo Scikit-learn: {model_name}\")\n",
    "    # Algumas classes de modelo podem precisar de hiperparâmetros específicos\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    elif model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    else: # Fallback para outros, embora tenhamos apenas RF e LR por enquanto\n",
    "        model = model_class(random_state=42)\n",
    "        \n",
    "    model.fit(X_train, y_exercise_train_flat)\n",
    "    \n",
    "    # Avaliar no conjunto de validação\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_exercise_val_flat, y_pred_val)\n",
    "    print(f\"   Acurácia de Validação para {model_name}: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Construção da string do nome do arquivo separadamente para evitar conflito de aspas\n",
    "    model_filename_suffix = f'best_{model_name.lower().replace(\" \", \"_\")}_model.pkl'\n",
    "    model_filepath = MODELS_DIR / model_filename_suffix\n",
    "\n",
    "    joblib.dump(model, model_filepath) # Usa o Path construído\n",
    "    print(f\"✅ Modelo '{model_name}' treinado e salvo em {model_filepath}\") # Usa o Path construído\n",
    "    return model, val_accuracy\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e validação\n",
    "X_train, X_val, y_exercise_train, y_exercise_val, y_form_train, y_form_val = train_test_split(\n",
    "    X_scaled, y_exercise, y_form, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Converter y_exercise_train/val de one-hot para rótulos flat para modelos Scikit-learn\n",
    "y_exercise_train_flat = np.argmax(y_exercise_train, axis=1)\n",
    "y_exercise_val_flat = np.argmax(y_exercise_val, axis=1)\n",
    "\n",
    "num_features = X_scaled.shape[1]\n",
    "num_exercise_classes = y_exercise.shape[1]\n",
    "\n",
    "# Dicionário para armazenar o desempenho de cada modelo\n",
    "model_performance = {}\n",
    "\n",
    "# --- Treinar e Avaliar o Modelo Keras MLP ---\n",
    "mlp_model_path = MODELS_DIR / 'best_mlp_model.h5'\n",
    "if os.path.exists(mlp_model_path):\n",
    "    print(f\"ℹ️ Modelo MLP pré-treinado encontrado. Carregando...\")\n",
    "    mlp_model = load_model(mlp_model_path)\n",
    "    print(f\"✅ Modelo MLP carregado.\")\n",
    "else:\n",
    "    print(f\"⚙️ Modelo MLP não encontrado. Iniciando construção e treinamento...\")\n",
    "    mlp_model = build_keras_mlp_model(num_features, num_exercise_classes)\n",
    "    \n",
    "    checkpoint_mlp = ModelCheckpoint(\n",
    "        mlp_model_path,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr_mlp = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stop_mlp = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Iniciando treinamento para o modelo MLP...\")\n",
    "    history_mlp = mlp_model.fit(\n",
    "        X_train,\n",
    "        {'exercise_output': y_exercise_train, 'form_output': y_form_train},\n",
    "        epochs=100000,\n",
    "        batch_size=1080,\n",
    "        validation_data=(X_val, {'exercise_output': y_exercise_val, 'form_output': y_form_val}),\n",
    "        callbacks=[checkpoint_mlp, reduce_lr_mlp, early_stop_mlp],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(f\"✅ Modelo MLP treinado e salvo em {mlp_model_path}\")\n",
    "\n",
    "# Avaliar o modelo MLP no conjunto de validação\n",
    "print(f\"Avaliando o modelo MLP no conjunto de validação...\")\n",
    "val_results_mlp = mlp_model.evaluate(X_val, {'exercise_output': y_exercise_val, 'form_output': y_form_val}, verbose=0)\n",
    "val_loss_mlp = val_results_mlp[0]\n",
    "val_exercise_accuracy_mlp = val_results_mlp[3] # Índice 3 para accuracy da exercise_output\n",
    "val_form_accuracy_mlp = val_results_mlp[4] # Índice 4 para accuracy da form_output\n",
    "model_performance['MLP Keras (Exercício & Forma)'] = {\n",
    "    'val_loss': val_loss_mlp,\n",
    "    'val_exercise_accuracy': val_exercise_accuracy_mlp,\n",
    "    'val_form_accuracy': val_form_accuracy_mlp,\n",
    "    'type': 'keras_multi_output',\n",
    "    'path': mlp_model_path\n",
    "}\n",
    "print(f\"MLP Keras - Perda de Validação: {val_loss_mlp:.4f}, Acurácia Exercício de Validação: {val_exercise_accuracy_mlp:.4f}, Acurácia Forma de Validação: {val_form_accuracy_mlp:.4f}\")\n",
    "\n",
    "\n",
    "# --- Treinar e Avaliar Modelos Scikit-learn ---\n",
    "# Random Forest\n",
    "rf_model, rf_val_accuracy = train_and_save_sklearn_model(RandomForestClassifier, \"Random Forest\", X_train, y_exercise_train_flat, X_val, y_exercise_val_flat)\n",
    "model_performance['Random Forest (Apenas Exercício)'] = {\n",
    "    'val_exercise_accuracy': rf_val_accuracy,\n",
    "    'type': 'sklearn_single_output',\n",
    "    'path': MODELS_DIR / f'best_random_forest_model.pkl' # Este já estava ok\n",
    "}\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model, lr_val_accuracy = train_and_save_sklearn_model(LogisticRegression, \"Regressão Logística\", X_train, y_exercise_train_flat, X_val, y_exercise_val_flat)\n",
    "model_performance['Regressão Logística (Apenas Exercício)'] = {\n",
    "    'val_exercise_accuracy': lr_val_accuracy,\n",
    "    'type': 'sklearn_single_output',\n",
    "    'path': MODELS_DIR / f'best_logistic_regression_model.pkl' # Este já estava ok\n",
    "}\n",
    "\n",
    "\n",
    "# --- Selecionar o Modelo Mais Eficiente ---\n",
    "best_model_for_exercise_classification_name = None\n",
    "highest_exercise_accuracy = -1\n",
    "\n",
    "print(\"\\n--- Resultados da Comparação de Modelos (Validação) ---\")\n",
    "for name, metrics in model_performance.items():\n",
    "    print(f\"Modelo: {name}\")\n",
    "    if 'val_loss' in metrics:\n",
    "        print(f\"  Perda de Validação: {metrics['val_loss']:.4f}\")\n",
    "    print(f\"  Acurácia de Exercício de Validação: {metrics['val_exercise_accuracy']:.4f}\")\n",
    "    if 'val_form_accuracy' in metrics:\n",
    "        print(f\"  Acurácia de Forma de Validação: {metrics['val_form_accuracy']:.4f}\")\n",
    "    \n",
    "    # Para a comparação de qual modelo é \"mais eficiente\" para o propósito do aplicativo,\n",
    "    # o modelo Keras MLP é o único que oferece detecção de forma.\n",
    "    # No entanto, para fins de relatório de comparação de acurácia de exercício,\n",
    "    # identificamos o de maior acurácia de exercício.\n",
    "    if metrics['val_exercise_accuracy'] > highest_exercise_accuracy:\n",
    "        highest_exercise_accuracy = metrics['val_exercise_accuracy']\n",
    "        best_model_for_exercise_classification_name = name\n",
    "\n",
    "print(f\"\\n🏆 Modelo com maior acurácia para Classificação de Exercício (com base na acurácia de validação): '{best_model_for_exercise_classification_name}' com acurácia: {highest_exercise_accuracy:.4f}\")\n",
    "\n",
    "# Para a aplicação em tempo real, o modelo Keras MLP é escolhido porque é o único\n",
    "# que fornece ambas as detecções: tipo de exercício E forma.\n",
    "final_model_to_use_name = \"MLP Keras (Exercício & Forma)\"\n",
    "final_model_to_use_path = model_performance[final_model_to_use_name]['path']\n",
    "final_model_type = model_performance[final_model_to_use_name]['type']\n",
    "\n",
    "print(f\"\\nEscolha final para aplicação em tempo real (devido à saída combinada de exercício e forma): '{final_model_to_use_name}'\")\n",
    "print(f\"Carregando este modelo para detecção em tempo real: {final_model_to_use_path}\")\n",
    "\n",
    "# Carregar o modelo selecionado final para detecção em tempo real\n",
    "if final_model_type == 'keras_multi_output':\n",
    "    model = load_model(final_model_to_use_path)\n",
    "    # Garantir que exercise_encoder e feature_scaler estejam carregados se não estiverem na memória\n",
    "    if 'exercise_encoder' not in locals() or 'feature_scaler' not in locals():\n",
    "        exercise_encoder = joblib.load(MODELS_DIR / 'exercise_encoder.pkl')\n",
    "        feature_scaler = joblib.load(MODELS_DIR / 'feature_scaler.pkl')\n",
    "else:\n",
    "    # Este caso não deve ser alcançado se o MLP Keras for sempre escolhido para tempo real\n",
    "    # mas incluído para completude caso a lógica mude.\n",
    "    model = joblib.load(final_model_to_use_path)\n",
    "    if 'exercise_encoder' not in locals() or 'feature_scaler' not in locals():\n",
    "        exercise_encoder = joblib.load(MODELS_DIR / 'exercise_encoder.pkl')\n",
    "        feature_scaler = joblib.load(MODELS_DIR / 'feature_scaler.pkl')\n",
    "    print(\"AVISO: Um modelo não-Keras multi-saída foi selecionado para tempo real. O feedback de forma pode ser limitado.\")\n",
    "\n",
    "\n",
    "# 6. Detecção de Forma de Exercícios em Tempo Real\n",
    "\n",
    "# Mapeamento para os rótulos de forma\n",
    "form_labels = ['Forma Correta', 'Forma Incorreta']\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detecção em tempo real\n",
    "# 'static_image_mode=False' para melhor desempenho em streams de vídeo.\n",
    "# O MediaPipe automaticamente usa a CPU neste ambiente.\n",
    "pose_live_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Função de predição em tempo real\n",
    "def detect_exercise_form(frame, pose_detector, model, feature_scaler, exercise_encoder, form_labels, model_type):\n",
    "    \"\"\"\n",
    "    Processa um frame da webcam para detectar a pose e prever o exercício e a forma.\n",
    "    Args:\n",
    "        frame (numpy.ndarray): O frame de vídeo da webcam.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado (executando em CPU).\n",
    "        model (tensorflow.keras.Model): O modelo de ML treinado (inferência em CPU).\n",
    "        feature_scaler (MinMaxScaler): O scaler usado para normalizar as features.\n",
    "        exercise_encoder (LabelEncoder): O encoder usado para os rótulos de exercício.\n",
    "        form_labels (list): Lista de rótulos para a forma (e.g., ['Correto', 'Incorreto']).\n",
    "        model_type (str): Tipo do modelo (e.g., 'keras_multi_output', 'sklearn_single_output').\n",
    "    Returns:\n",
    "        numpy.ndarray: O frame com as informações de pose e feedback.\n",
    "    \"\"\"\n",
    "    # Converter o frame para RGB para o MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Melhorar o desempenho (opcional): marcar a imagem como não gravável para passar por referência.\n",
    "    image_rgb.flags.writeable = False\n",
    "    \n",
    "    # Processar o frame para estimativa de pose (executa na CPU)\n",
    "    results = pose_detector.process(image_rgb)\n",
    "    \n",
    "    # Marcar a imagem como gravável novamente para desenhar as anotações\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # Voltar para BGR para o OpenCV\n",
    "    \n",
    "    feedback_text = \"Nenhuma pose detectada.\"\n",
    "    text_color = (255, 255, 255) # Cor padrão branca para feedback\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Desenhar as landmarks da pose no frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_bgr,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Extrair features da pose detectada no frame atual\n",
    "        # Replicamos a lógica de extract_pose_features para o frame atual\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        try:\n",
    "            # Coletar as mesmas features que foram usadas no treinamento\n",
    "            live_features = np.array([\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                )\n",
    "            ])\n",
    "            \n",
    "            # Normalizar as features usando o scaler treinado\n",
    "            live_features_scaled = feature_scaler.transform(live_features.reshape(1, -1))\n",
    "            \n",
    "            if model_type == 'keras_multi_output':\n",
    "                predictions = model.predict(live_features_scaled, verbose=0)\n",
    "                exercise_probs = predictions[0][0]\n",
    "                form_probs = predictions[1][0]\n",
    "                \n",
    "                predicted_exercise_idx = np.argmax(exercise_probs)\n",
    "                predicted_exercise = exercise_encoder.inverse_transform([predicted_exercise_idx])[0]\n",
    "                \n",
    "                predicted_form_idx = np.argmax(form_probs)\n",
    "                predicted_form = form_labels[predicted_form_idx]\n",
    "                \n",
    "                feedback_text = f\"Exercicio: {predicted_exercise} ({exercise_probs[predicted_exercise_idx]*100:.1f}%) | Forma: {predicted_form} ({form_probs[predicted_form_idx]*100:.1f}%)\"\n",
    "                \n",
    "                if predicted_form_idx == 1: # Se a forma for incorreta\n",
    "                    text_color = (0, 0, 255) # Vermelho\n",
    "                    feedback_text += \" -> Ajuste sua postura!\"\n",
    "                else:\n",
    "                    text_color = (0, 255, 0) # Verde para correto\n",
    "            else: # Modelos Scikit-learn (ou qualquer outro modelo de saída única para exercício)\n",
    "                # Estes modelos predizem apenas o tipo de exercício.\n",
    "                # Assumindo que os modelos sklearn foram selecionados por sua maior acurácia de exercício.\n",
    "                # Para a 'forma', vamos assumir 'Correta' por padrão, já que eles não a preveem explicitamente.\n",
    "                exercise_pred_id = model.predict(live_features_scaled)[0]\n",
    "                exercise_probs_all = model.predict_proba(live_features_scaled)[0] # Obter probabilidades para todas as classes\n",
    "                \n",
    "                predicted_exercise = exercise_encoder.inverse_transform([exercise_pred_id])[0]\n",
    "                confidence = exercise_probs_all[exercise_pred_id]\n",
    "                \n",
    "                # Forma padrão para correta para modelos de saída única, pois eles não preveem a forma explicitamente\n",
    "                predicted_form = \"Forma Correta\" \n",
    "                predicted_form_idx = 0 # Padrão para índice de forma correta\n",
    "                form_probs = np.array([1.0, 0.0]) # Assumir 100% de confiança na forma correta para fins de exibição\n",
    "                \n",
    "                feedback_text = f\"Exercicio: {predicted_exercise} ({confidence*100:.1f}%) | Forma: {predicted_form}\"\n",
    "                text_color = (0, 255, 0) # Sempre verde se a forma for assumida como correta\n",
    "\n",
    "        except Exception as e:\n",
    "            feedback_text = f\"Erro na predição: {e}\"\n",
    "            text_color = (0, 165, 255) # Laranja para erro\n",
    "            print(f\"Erro na detecção: {e}\")\n",
    "\n",
    "    # Exibir o feedback no frame\n",
    "    cv2.putText(image_bgr, feedback_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image_bgr\n",
    "\n",
    "# Loop da Webcam para Detecção em Tempo Real\n",
    "\n",
    "# Inicializar a captura de vídeo da webcam (0 é geralmente a câmera padrão)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Não foi possível acessar a webcam. Verifique se ela está conectada e não está em uso.\")\n",
    "else:\n",
    "    # Definir resolução da webcam para melhor qualidade\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    window_name = 'Detecao de Forma de Exercicios (CPU) - Pressione Q para Sair'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    print(\"\\n✅ Webcam ativada. Pressione 'Q' para sair da janela de visualização.\")\n",
    "    # Exibir a hora atual conforme as instruções\n",
    "    current_utc_time = time.gmtime()\n",
    "    current_brasilia_time = time.localtime(time.time() - 3 * 3600) # UTC-3\n",
    "    print(f\"UTC: {time.strftime('%d/%m/%Y %H:%M:%S', current_utc_time)} (UTC)\")\n",
    "    print(f\"Brasília: {time.strftime('%d/%m/%Y %H:%M:%S', current_brasilia_time)} (UTC-3)\")\n",
    "    print(\"\\n⚠️ A performance pode ser limitada pela sua CPU. Para melhor experiência, considere GPUs dedicadas.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"❌ Falha ao ler frame da webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Espelhar o frame para que a visualização seja mais intuitiva (como um espelho)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Chamar a função de detecção e obter o frame com feedback\n",
    "            output_frame = detect_exercise_form(frame, pose_live_detector, model, feature_scaler, exercise_encoder, form_labels, final_model_type)\n",
    "            \n",
    "            # Mostrar o frame na janela\n",
    "            cv2.imshow(window_name, output_frame)\n",
    "            \n",
    "            # Verificar se a janela foi fechada pelo usuário\n",
    "            if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "                \n",
    "            # Sair do loop se a tecla 'q' for pressionada\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        # Liberar os recursos da webcam e fechar todas as janelas do OpenCV\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"✅ Programa finalizado. Recursos liberados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39cb7b-1eb5-46e2-b434-d7a18578882e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
