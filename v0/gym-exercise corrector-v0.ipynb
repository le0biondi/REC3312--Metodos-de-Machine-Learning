{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde01d2f-77b1-44ab-8cf0-636e042d870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Nenhuma GPU NVIDIA detectada ou configurada. Todas as operações de Machine Learning utilizarão a CPU.\n",
      "✅ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\n",
      "✅ Funções de extração de pose, cálculo de ângulos e geração de erros sintéticos prontas.\n",
      "ℹ️ Dataset 'workout-exercises-images' já existe em workout-exercises-images. Pulando download.\n",
      "\n",
      "✅ Tipos de exercícios encontrados no dataset: ['barbell biceps curl', 'bench press', 'chest fly machine', 'deadlift', 'decline bench press', 'hammer curl', 'hip thrust', 'incline bench press', 'lat pulldown', 'lateral raises', 'leg extension', 'leg raises', 'plank', 'pull up', 'push up', 'romanian deadlift', 'russian twist', 'shoulder press', 'squat', 't bar row', 'tricep dips', 'tricep pushdown']\n",
      "\n",
      "⚙️ Processando 705 imagens para o exercício: barbell biceps curl\n",
      "   Amostrando 200 imagens de 705 disponíveis para barbell biceps curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de barbell biceps curl: 100%|██████████████████████████████████| 200/200 [00:10<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 625 imagens para o exercício: bench press\n",
      "   Amostrando 200 imagens de 625 disponíveis para bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de bench press: 100%|██████████████████████████████████████████| 200/200 [00:10<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 527 imagens para o exercício: chest fly machine\n",
      "   Amostrando 200 imagens de 527 disponíveis para chest fly machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de chest fly machine: 100%|████████████████████████████████████| 200/200 [00:12<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 530 imagens para o exercício: deadlift\n",
      "   Amostrando 200 imagens de 530 disponíveis para deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de deadlift: 100%|█████████████████████████████████████████████| 200/200 [00:11<00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 514 imagens para o exercício: decline bench press\n",
      "   Amostrando 200 imagens de 514 disponíveis para decline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de decline bench press: 100%|██████████████████████████████████| 200/200 [00:10<00:00, 19.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 546 imagens para o exercício: hammer curl\n",
      "   Amostrando 200 imagens de 546 disponíveis para hammer curl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hammer curl: 100%|██████████████████████████████████████████| 200/200 [00:11<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 557 imagens para o exercício: hip thrust\n",
      "   Amostrando 200 imagens de 557 disponíveis para hip thrust.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de hip thrust: 100%|███████████████████████████████████████████| 200/200 [00:12<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 729 imagens para o exercício: incline bench press\n",
      "   Amostrando 200 imagens de 729 disponíveis para incline bench press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de incline bench press: 100%|██████████████████████████████████| 200/200 [00:11<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 646 imagens para o exercício: lat pulldown\n",
      "   Amostrando 200 imagens de 646 disponíveis para lat pulldown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lat pulldown: 100%|█████████████████████████████████████████| 200/200 [00:11<00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 843 imagens para o exercício: lateral raises\n",
      "   Amostrando 200 imagens de 843 disponíveis para lateral raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de lateral raises: 100%|███████████████████████████████████████| 200/200 [00:12<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 586 imagens para o exercício: leg extension\n",
      "   Amostrando 200 imagens de 586 disponíveis para leg extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg extension: 100%|████████████████████████████████████████| 200/200 [00:11<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 514 imagens para o exercício: leg raises\n",
      "   Amostrando 200 imagens de 514 disponíveis para leg raises.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de leg raises: 100%|███████████████████████████████████████████| 200/200 [00:12<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 993 imagens para o exercício: plank\n",
      "   Amostrando 200 imagens de 993 disponíveis para plank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de plank: 100%|████████████████████████████████████████████████| 200/200 [00:12<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 615 imagens para o exercício: pull up\n",
      "   Amostrando 200 imagens de 615 disponíveis para pull up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de pull up: 100%|██████████████████████████████████████████████| 200/200 [00:11<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 601 imagens para o exercício: push up\n",
      "   Amostrando 200 imagens de 601 disponíveis para push up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de push up: 100%|██████████████████████████████████████████████| 200/200 [00:12<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 555 imagens para o exercício: romanian deadlift\n",
      "   Amostrando 200 imagens de 555 disponíveis para romanian deadlift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de romanian deadlift: 100%|████████████████████████████████████| 200/200 [00:11<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 522 imagens para o exercício: russian twist\n",
      "   Amostrando 200 imagens de 522 disponíveis para russian twist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de russian twist: 100%|████████████████████████████████████████| 200/200 [00:12<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 512 imagens para o exercício: shoulder press\n",
      "   Amostrando 200 imagens de 512 disponíveis para shoulder press.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de shoulder press: 100%|███████████████████████████████████████| 200/200 [00:12<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 742 imagens para o exercício: squat\n",
      "   Amostrando 200 imagens de 742 disponíveis para squat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de squat: 100%|████████████████████████████████████████████████| 200/200 [00:12<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 668 imagens para o exercício: t bar row\n",
      "   Amostrando 200 imagens de 668 disponíveis para t bar row.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de t bar row: 100%|████████████████████████████████████████████| 200/200 [00:12<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 698 imagens para o exercício: tricep dips\n",
      "   Amostrando 200 imagens de 698 disponíveis para tricep dips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep dips: 100%|██████████████████████████████████████████| 200/200 [00:12<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ Processando 625 imagens para o exercício: tricep pushdown\n",
      "   Amostrando 200 imagens de 625 disponíveis para tricep pushdown.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extraindo features de tricep pushdown: 100%|██████████████████████████████████████| 200/200 [00:11<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total de 3906 amostras processadas.\n",
      "   Shape das features (X): (3906, 8)\n",
      "   Shape dos rótulos de exercício (y_exercise): (3906, 22)\n",
      "   Shape dos rótulos de forma (y_form): (3906, 2)\n",
      "   Exercícios detectados: ['barbell biceps curl' 'bench press' 'chest fly machine' 'deadlift'\n",
      " 'decline bench press' 'hammer curl' 'hip thrust' 'incline bench press'\n",
      " 'lat pulldown' 'lateral raises' 'leg extension' 'leg raises' 'plank'\n",
      " 'pull up' 'push up' 'romanian deadlift' 'russian twist' 'shoulder press'\n",
      " 'squat' 't bar row' 'tricep dips' 'tricep pushdown']\n",
      "✅ Pré-processamento de dados de exercícios concluído e escalers salvos.\n",
      "⚙️ Modelo não encontrado. Iniciando construção e treinamento de novo modelo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,608</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalization_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ exercise_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,838</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ form_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               │           \u001b[38;5;34m4,608\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               │           \u001b[38;5;34m2,048\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ batch_normalization[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m131,328\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │           \u001b[38;5;34m1,024\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ batch_normalization_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ batch_normalization_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │             \u001b[38;5;34m512\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ batch_normalization_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ exercise_output (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m)                │           \u001b[38;5;34m2,838\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ form_output (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m258\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">175,512</span> (685.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m175,512\u001b[0m (685.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">173,720</span> (678.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m173,720\u001b[0m (678.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.1687 - exercise_output_loss: 3.0621 - form_output_accuracy: 0.4956 - form_output_loss: 1.0154 - loss: 2.4481\n",
      "Epoch 1: val_loss improved from inf to 2.36014, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - exercise_output_accuracy: 0.1736 - exercise_output_loss: 3.0323 - form_output_accuracy: 0.4972 - form_output_loss: 1.0064 - loss: 2.4246 - val_exercise_output_accuracy: 0.1049 - val_exercise_output_loss: 3.0841 - val_form_output_accuracy: 0.5959 - val_form_output_loss: 0.6741 - val_loss: 2.3601 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.2956 - exercise_output_loss: 2.4005 - form_output_accuracy: 0.5360 - form_output_loss: 0.8150 - loss: 1.9248\n",
      "Epoch 2: val_loss did not improve from 2.36014\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.2956 - exercise_output_loss: 2.3925 - form_output_accuracy: 0.5367 - form_output_loss: 0.8140 - loss: 1.9189 - val_exercise_output_accuracy: 0.0652 - val_exercise_output_loss: 3.4001 - val_form_output_accuracy: 0.5959 - val_form_output_loss: 0.6776 - val_loss: 2.5798 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3307 - exercise_output_loss: 2.1167 - form_output_accuracy: 0.5425 - form_output_loss: 0.7672 - loss: 1.7119\n",
      "Epoch 3: val_loss did not improve from 2.36014\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.3308 - exercise_output_loss: 2.1184 - form_output_accuracy: 0.5426 - form_output_loss: 0.7667 - loss: 1.7129 - val_exercise_output_accuracy: 0.0959 - val_exercise_output_loss: 3.3171 - val_form_output_accuracy: 0.5921 - val_form_output_loss: 0.6815 - val_loss: 2.5221 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3524 - exercise_output_loss: 2.0368 - form_output_accuracy: 0.5475 - form_output_loss: 0.7326 - loss: 1.6456\n",
      "Epoch 4: val_loss improved from 2.36014 to 2.26614, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.3524 - exercise_output_loss: 2.0378 - form_output_accuracy: 0.5484 - form_output_loss: 0.7317 - loss: 1.6460 - val_exercise_output_accuracy: 0.1125 - val_exercise_output_loss: 2.9545 - val_form_output_accuracy: 0.5921 - val_form_output_loss: 0.6769 - val_loss: 2.2661 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3729 - exercise_output_loss: 2.0142 - form_output_accuracy: 0.5458 - form_output_loss: 0.7264 - loss: 1.6278\n",
      "Epoch 5: val_loss improved from 2.26614 to 1.89007, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.3727 - exercise_output_loss: 2.0145 - form_output_accuracy: 0.5459 - form_output_loss: 0.7262 - loss: 1.6280 - val_exercise_output_accuracy: 0.2327 - val_exercise_output_loss: 2.4139 - val_form_output_accuracy: 0.5716 - val_form_output_loss: 0.6872 - val_loss: 1.8901 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3665 - exercise_output_loss: 2.0061 - form_output_accuracy: 0.5611 - form_output_loss: 0.6961 - loss: 1.6131\n",
      "Epoch 6: val_loss improved from 1.89007 to 1.63294, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.3664 - exercise_output_loss: 2.0026 - form_output_accuracy: 0.5610 - form_output_loss: 0.6967 - loss: 1.6108 - val_exercise_output_accuracy: 0.3363 - val_exercise_output_loss: 2.0474 - val_form_output_accuracy: 0.5908 - val_form_output_loss: 0.6759 - val_loss: 1.6329 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3556 - exercise_output_loss: 1.9995 - form_output_accuracy: 0.5662 - form_output_loss: 0.6947 - loss: 1.6080\n",
      "Epoch 7: val_loss improved from 1.63294 to 1.50305, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.3572 - exercise_output_loss: 1.9954 - form_output_accuracy: 0.5669 - form_output_loss: 0.6944 - loss: 1.6051 - val_exercise_output_accuracy: 0.4297 - val_exercise_output_loss: 1.8621 - val_form_output_accuracy: 0.5934 - val_form_output_loss: 0.6764 - val_loss: 1.5031 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.3891 - exercise_output_loss: 1.8721 - form_output_accuracy: 0.5786 - form_output_loss: 0.6859 - loss: 1.5163\n",
      "Epoch 8: val_loss improved from 1.50305 to 1.42288, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.3889 - exercise_output_loss: 1.8728 - form_output_accuracy: 0.5783 - form_output_loss: 0.6860 - loss: 1.5168 - val_exercise_output_accuracy: 0.4655 - val_exercise_output_loss: 1.7356 - val_form_output_accuracy: 0.5767 - val_form_output_loss: 0.6819 - val_loss: 1.4229 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.3981 - exercise_output_loss: 1.8940 - form_output_accuracy: 0.5566 - form_output_loss: 0.6971 - loss: 1.5349\n",
      "Epoch 9: val_loss did not improve from 1.42288\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.3973 - exercise_output_loss: 1.8911 - form_output_accuracy: 0.5578 - form_output_loss: 0.6963 - loss: 1.5327 - val_exercise_output_accuracy: 0.4604 - val_exercise_output_loss: 1.7737 - val_form_output_accuracy: 0.5831 - val_form_output_loss: 0.6792 - val_loss: 1.4461 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4057 - exercise_output_loss: 1.8261 - form_output_accuracy: 0.5993 - form_output_loss: 0.6775 - loss: 1.4815\n",
      "Epoch 10: val_loss improved from 1.42288 to 1.40463, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4050 - exercise_output_loss: 1.8286 - form_output_accuracy: 0.5984 - form_output_loss: 0.6777 - loss: 1.4833 - val_exercise_output_accuracy: 0.4629 - val_exercise_output_loss: 1.7142 - val_form_output_accuracy: 0.5934 - val_form_output_loss: 0.6784 - val_loss: 1.4046 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4083 - exercise_output_loss: 1.8791 - form_output_accuracy: 0.5977 - form_output_loss: 0.6771 - loss: 1.5185\n",
      "Epoch 11: val_loss did not improve from 1.40463\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4082 - exercise_output_loss: 1.8785 - form_output_accuracy: 0.5974 - form_output_loss: 0.6772 - loss: 1.5181 - val_exercise_output_accuracy: 0.4616 - val_exercise_output_loss: 1.7144 - val_form_output_accuracy: 0.5857 - val_form_output_loss: 0.6748 - val_loss: 1.4048 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4085 - exercise_output_loss: 1.8377 - form_output_accuracy: 0.5933 - form_output_loss: 0.6710 - loss: 1.4877\n",
      "Epoch 12: val_loss did not improve from 1.40463\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4087 - exercise_output_loss: 1.8373 - form_output_accuracy: 0.5931 - form_output_loss: 0.6711 - loss: 1.4875 - val_exercise_output_accuracy: 0.4425 - val_exercise_output_loss: 1.7977 - val_form_output_accuracy: 0.5857 - val_form_output_loss: 0.6760 - val_loss: 1.4615 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4197 - exercise_output_loss: 1.7862 - form_output_accuracy: 0.6023 - form_output_loss: 0.6701 - loss: 1.4514\n",
      "Epoch 13: val_loss improved from 1.40463 to 1.37983, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4201 - exercise_output_loss: 1.7862 - form_output_accuracy: 0.6019 - form_output_loss: 0.6704 - loss: 1.4514 - val_exercise_output_accuracy: 0.4527 - val_exercise_output_loss: 1.6816 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6734 - val_loss: 1.3798 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4206 - exercise_output_loss: 1.7773 - form_output_accuracy: 0.6089 - form_output_loss: 0.6772 - loss: 1.4472\n",
      "Epoch 14: val_loss did not improve from 1.37983\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4206 - exercise_output_loss: 1.7769 - form_output_accuracy: 0.6080 - form_output_loss: 0.6774 - loss: 1.4471 - val_exercise_output_accuracy: 0.4642 - val_exercise_output_loss: 1.7337 - val_form_output_accuracy: 0.5985 - val_form_output_loss: 0.6765 - val_loss: 1.4161 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4126 - exercise_output_loss: 1.7728 - form_output_accuracy: 0.5867 - form_output_loss: 0.6775 - loss: 1.4442\n",
      "Epoch 15: val_loss did not improve from 1.37983\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4132 - exercise_output_loss: 1.7721 - form_output_accuracy: 0.5871 - form_output_loss: 0.6775 - loss: 1.4437 - val_exercise_output_accuracy: 0.4680 - val_exercise_output_loss: 1.7094 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6712 - val_loss: 1.3941 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4366 - exercise_output_loss: 1.7028 - form_output_accuracy: 0.5980 - form_output_loss: 0.6741 - loss: 1.3942\n",
      "Epoch 16: val_loss did not improve from 1.37983\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4365 - exercise_output_loss: 1.7034 - form_output_accuracy: 0.5979 - form_output_loss: 0.6741 - loss: 1.3946 - val_exercise_output_accuracy: 0.4591 - val_exercise_output_loss: 1.7151 - val_form_output_accuracy: 0.5895 - val_form_output_loss: 0.6718 - val_loss: 1.4035 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4392 - exercise_output_loss: 1.7637 - form_output_accuracy: 0.5974 - form_output_loss: 0.6735 - loss: 1.4366\n",
      "Epoch 17: val_loss improved from 1.37983 to 1.34781, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4392 - exercise_output_loss: 1.7625 - form_output_accuracy: 0.5974 - form_output_loss: 0.6736 - loss: 1.4359 - val_exercise_output_accuracy: 0.4936 - val_exercise_output_loss: 1.6335 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6763 - val_loss: 1.3478 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4366 - exercise_output_loss: 1.7061 - form_output_accuracy: 0.5906 - form_output_loss: 0.6778 - loss: 1.3976\n",
      "Epoch 18: val_loss did not improve from 1.34781\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4366 - exercise_output_loss: 1.7061 - form_output_accuracy: 0.5910 - form_output_loss: 0.6777 - loss: 1.3976 - val_exercise_output_accuracy: 0.5064 - val_exercise_output_loss: 1.6495 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6730 - val_loss: 1.3551 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.4673 - exercise_output_loss: 1.6461 - form_output_accuracy: 0.5966 - form_output_loss: 0.6790 - loss: 1.3560\n",
      "Epoch 19: val_loss did not improve from 1.34781\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4670 - exercise_output_loss: 1.6468 - form_output_accuracy: 0.5966 - form_output_loss: 0.6790 - loss: 1.3565 - val_exercise_output_accuracy: 0.4604 - val_exercise_output_loss: 1.6948 - val_form_output_accuracy: 0.5767 - val_form_output_loss: 0.6741 - val_loss: 1.3863 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.4400 - exercise_output_loss: 1.6949 - form_output_accuracy: 0.5876 - form_output_loss: 0.6817 - loss: 1.3909\n",
      "Epoch 20: val_loss improved from 1.34781 to 1.34735, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4412 - exercise_output_loss: 1.6945 - form_output_accuracy: 0.5885 - form_output_loss: 0.6812 - loss: 1.3905 - val_exercise_output_accuracy: 0.4847 - val_exercise_output_loss: 1.6393 - val_form_output_accuracy: 0.5946 - val_form_output_loss: 0.6707 - val_loss: 1.3474 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4395 - exercise_output_loss: 1.6865 - form_output_accuracy: 0.5954 - form_output_loss: 0.6720 - loss: 1.3822\n",
      "Epoch 21: val_loss did not improve from 1.34735\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4401 - exercise_output_loss: 1.6863 - form_output_accuracy: 0.5957 - form_output_loss: 0.6719 - loss: 1.3820 - val_exercise_output_accuracy: 0.4783 - val_exercise_output_loss: 1.6554 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6748 - val_loss: 1.3588 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4639 - exercise_output_loss: 1.6365 - form_output_accuracy: 0.5981 - form_output_loss: 0.6764 - loss: 1.3485\n",
      "Epoch 22: val_loss did not improve from 1.34735\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4637 - exercise_output_loss: 1.6379 - form_output_accuracy: 0.5985 - form_output_loss: 0.6764 - loss: 1.3495 - val_exercise_output_accuracy: 0.4680 - val_exercise_output_loss: 1.7286 - val_form_output_accuracy: 0.5831 - val_form_output_loss: 0.6804 - val_loss: 1.4112 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.4723 - exercise_output_loss: 1.5982 - form_output_accuracy: 0.6028 - form_output_loss: 0.6735 - loss: 1.3208\n",
      "Epoch 23: val_loss improved from 1.34735 to 1.34376, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4722 - exercise_output_loss: 1.5986 - form_output_accuracy: 0.6029 - form_output_loss: 0.6735 - loss: 1.3210 - val_exercise_output_accuracy: 0.4898 - val_exercise_output_loss: 1.6367 - val_form_output_accuracy: 0.5985 - val_form_output_loss: 0.6721 - val_loss: 1.3438 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.4640 - exercise_output_loss: 1.6282 - form_output_accuracy: 0.6090 - form_output_loss: 0.6715 - loss: 1.3412\n",
      "Epoch 24: val_loss did not improve from 1.34376\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4641 - exercise_output_loss: 1.6288 - form_output_accuracy: 0.6090 - form_output_loss: 0.6716 - loss: 1.3416 - val_exercise_output_accuracy: 0.4591 - val_exercise_output_loss: 1.6662 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6729 - val_loss: 1.3667 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4840 - exercise_output_loss: 1.5732 - form_output_accuracy: 0.5986 - form_output_loss: 0.6684 - loss: 1.3018\n",
      "Epoch 25: val_loss improved from 1.34376 to 1.31796, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4833 - exercise_output_loss: 1.5757 - form_output_accuracy: 0.5986 - form_output_loss: 0.6685 - loss: 1.3035 - val_exercise_output_accuracy: 0.5102 - val_exercise_output_loss: 1.5892 - val_form_output_accuracy: 0.5870 - val_form_output_loss: 0.6764 - val_loss: 1.3180 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.4750 - exercise_output_loss: 1.6089 - form_output_accuracy: 0.6101 - form_output_loss: 0.6695 - loss: 1.3271\n",
      "Epoch 26: val_loss improved from 1.31796 to 1.30778, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4750 - exercise_output_loss: 1.6092 - form_output_accuracy: 0.6101 - form_output_loss: 0.6694 - loss: 1.3273 - val_exercise_output_accuracy: 0.5320 - val_exercise_output_loss: 1.5777 - val_form_output_accuracy: 0.6138 - val_form_output_loss: 0.6779 - val_loss: 1.3078 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4752 - exercise_output_loss: 1.5945 - form_output_accuracy: 0.5875 - form_output_loss: 0.6761 - loss: 1.3190\n",
      "Epoch 27: val_loss did not improve from 1.30778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4760 - exercise_output_loss: 1.5943 - form_output_accuracy: 0.5890 - form_output_loss: 0.6758 - loss: 1.3187 - val_exercise_output_accuracy: 0.4450 - val_exercise_output_loss: 1.7207 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6705 - val_loss: 1.4019 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5010 - exercise_output_loss: 1.5464 - form_output_accuracy: 0.5913 - form_output_loss: 0.6741 - loss: 1.2847\n",
      "Epoch 28: val_loss did not improve from 1.30778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4995 - exercise_output_loss: 1.5498 - form_output_accuracy: 0.5919 - form_output_loss: 0.6739 - loss: 1.2870 - val_exercise_output_accuracy: 0.5064 - val_exercise_output_loss: 1.5967 - val_form_output_accuracy: 0.5831 - val_form_output_loss: 0.6767 - val_loss: 1.3238 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4971 - exercise_output_loss: 1.5391 - form_output_accuracy: 0.6137 - form_output_loss: 0.6647 - loss: 1.2767\n",
      "Epoch 29: val_loss did not improve from 1.30778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4966 - exercise_output_loss: 1.5411 - form_output_accuracy: 0.6132 - form_output_loss: 0.6650 - loss: 1.2783 - val_exercise_output_accuracy: 0.4731 - val_exercise_output_loss: 1.7015 - val_form_output_accuracy: 0.5882 - val_form_output_loss: 0.6763 - val_loss: 1.3869 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4876 - exercise_output_loss: 1.5400 - form_output_accuracy: 0.5938 - form_output_loss: 0.6759 - loss: 1.2808\n",
      "Epoch 30: val_loss did not improve from 1.30778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.4876 - exercise_output_loss: 1.5405 - form_output_accuracy: 0.5938 - form_output_loss: 0.6759 - loss: 1.2811 - val_exercise_output_accuracy: 0.4987 - val_exercise_output_loss: 1.6150 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6742 - val_loss: 1.3338 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4958 - exercise_output_loss: 1.5348 - form_output_accuracy: 0.5911 - form_output_loss: 0.6767 - loss: 1.2774\n",
      "Epoch 31: val_loss did not improve from 1.30778\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4945 - exercise_output_loss: 1.5383 - form_output_accuracy: 0.5915 - form_output_loss: 0.6764 - loss: 1.2797 - val_exercise_output_accuracy: 0.5256 - val_exercise_output_loss: 1.6052 - val_form_output_accuracy: 0.6010 - val_form_output_loss: 0.6727 - val_loss: 1.3294 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5042 - exercise_output_loss: 1.4966 - form_output_accuracy: 0.6106 - form_output_loss: 0.6686 - loss: 1.2482\n",
      "Epoch 32: val_loss improved from 1.30778 to 1.30596, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5038 - exercise_output_loss: 1.4981 - form_output_accuracy: 0.6107 - form_output_loss: 0.6687 - loss: 1.2493 - val_exercise_output_accuracy: 0.5102 - val_exercise_output_loss: 1.5809 - val_form_output_accuracy: 0.5921 - val_form_output_loss: 0.6760 - val_loss: 1.3060 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4886 - exercise_output_loss: 1.5239 - form_output_accuracy: 0.6059 - form_output_loss: 0.6713 - loss: 1.2681\n",
      "Epoch 33: val_loss improved from 1.30596 to 1.29219, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4893 - exercise_output_loss: 1.5228 - form_output_accuracy: 0.6057 - form_output_loss: 0.6714 - loss: 1.2674 - val_exercise_output_accuracy: 0.5230 - val_exercise_output_loss: 1.5604 - val_form_output_accuracy: 0.5908 - val_form_output_loss: 0.6746 - val_loss: 1.2922 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5086 - exercise_output_loss: 1.5376 - form_output_accuracy: 0.6140 - form_output_loss: 0.6621 - loss: 1.2750\n",
      "Epoch 34: val_loss improved from 1.29219 to 1.25481, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5084 - exercise_output_loss: 1.5367 - form_output_accuracy: 0.6138 - form_output_loss: 0.6624 - loss: 1.2744 - val_exercise_output_accuracy: 0.5230 - val_exercise_output_loss: 1.5049 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6731 - val_loss: 1.2548 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5071 - exercise_output_loss: 1.4739 - form_output_accuracy: 0.6068 - form_output_loss: 0.6681 - loss: 1.2321\n",
      "Epoch 35: val_loss did not improve from 1.25481\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5073 - exercise_output_loss: 1.4739 - form_output_accuracy: 0.6068 - form_output_loss: 0.6682 - loss: 1.2321 - val_exercise_output_accuracy: 0.5384 - val_exercise_output_loss: 1.5285 - val_form_output_accuracy: 0.5946 - val_form_output_loss: 0.6779 - val_loss: 1.2725 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5168 - exercise_output_loss: 1.4712 - form_output_accuracy: 0.6034 - form_output_loss: 0.6692 - loss: 1.2306\n",
      "Epoch 36: val_loss improved from 1.25481 to 1.24575, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5167 - exercise_output_loss: 1.4714 - form_output_accuracy: 0.6036 - form_output_loss: 0.6692 - loss: 1.2307 - val_exercise_output_accuracy: 0.5396 - val_exercise_output_loss: 1.4913 - val_form_output_accuracy: 0.5946 - val_form_output_loss: 0.6736 - val_loss: 1.2458 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5269 - exercise_output_loss: 1.4677 - form_output_accuracy: 0.6080 - form_output_loss: 0.6677 - loss: 1.2277\n",
      "Epoch 37: val_loss improved from 1.24575 to 1.22982, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5261 - exercise_output_loss: 1.4675 - form_output_accuracy: 0.6079 - form_output_loss: 0.6675 - loss: 1.2275 - val_exercise_output_accuracy: 0.5537 - val_exercise_output_loss: 1.4661 - val_form_output_accuracy: 0.5972 - val_form_output_loss: 0.6769 - val_loss: 1.2298 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5303 - exercise_output_loss: 1.4437 - form_output_accuracy: 0.6101 - form_output_loss: 0.6651 - loss: 1.2101\n",
      "Epoch 38: val_loss did not improve from 1.22982\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5298 - exercise_output_loss: 1.4452 - form_output_accuracy: 0.6103 - form_output_loss: 0.6652 - loss: 1.2112 - val_exercise_output_accuracy: 0.5563 - val_exercise_output_loss: 1.4968 - val_form_output_accuracy: 0.5921 - val_form_output_loss: 0.6758 - val_loss: 1.2510 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5297 - exercise_output_loss: 1.4454 - form_output_accuracy: 0.6157 - form_output_loss: 0.6642 - loss: 1.2111\n",
      "Epoch 39: val_loss did not improve from 1.22982\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5288 - exercise_output_loss: 1.4467 - form_output_accuracy: 0.6152 - form_output_loss: 0.6644 - loss: 1.2120 - val_exercise_output_accuracy: 0.5524 - val_exercise_output_loss: 1.4827 - val_form_output_accuracy: 0.5985 - val_form_output_loss: 0.6778 - val_loss: 1.2419 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5136 - exercise_output_loss: 1.5174 - form_output_accuracy: 0.6052 - form_output_loss: 0.6712 - loss: 1.2635\n",
      "Epoch 40: val_loss did not improve from 1.22982\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5141 - exercise_output_loss: 1.5155 - form_output_accuracy: 0.6053 - form_output_loss: 0.6712 - loss: 1.2622 - val_exercise_output_accuracy: 0.5537 - val_exercise_output_loss: 1.4785 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6750 - val_loss: 1.2386 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5220 - exercise_output_loss: 1.4580 - form_output_accuracy: 0.6198 - form_output_loss: 0.6603 - loss: 1.2187\n",
      "Epoch 41: val_loss improved from 1.22982 to 1.21587, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - exercise_output_accuracy: 0.5218 - exercise_output_loss: 1.4587 - form_output_accuracy: 0.6193 - form_output_loss: 0.6606 - loss: 1.2193 - val_exercise_output_accuracy: 0.5601 - val_exercise_output_loss: 1.4484 - val_form_output_accuracy: 0.5844 - val_form_output_loss: 0.6778 - val_loss: 1.2159 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.4948 - exercise_output_loss: 1.4988 - form_output_accuracy: 0.6171 - form_output_loss: 0.6618 - loss: 1.2477\n",
      "Epoch 42: val_loss did not improve from 1.21587\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.4950 - exercise_output_loss: 1.4983 - form_output_accuracy: 0.6169 - form_output_loss: 0.6620 - loss: 1.2474 - val_exercise_output_accuracy: 0.5550 - val_exercise_output_loss: 1.4568 - val_form_output_accuracy: 0.5895 - val_form_output_loss: 0.6787 - val_loss: 1.2223 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5301 - exercise_output_loss: 1.4187 - form_output_accuracy: 0.6068 - form_output_loss: 0.6639 - loss: 1.1922\n",
      "Epoch 43: val_loss did not improve from 1.21587\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5294 - exercise_output_loss: 1.4203 - form_output_accuracy: 0.6069 - form_output_loss: 0.6641 - loss: 1.1934 - val_exercise_output_accuracy: 0.5499 - val_exercise_output_loss: 1.4871 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6744 - val_loss: 1.2420 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5255 - exercise_output_loss: 1.4305 - form_output_accuracy: 0.6004 - form_output_loss: 0.6693 - loss: 1.2021\n",
      "Epoch 44: val_loss did not improve from 1.21587\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5255 - exercise_output_loss: 1.4307 - form_output_accuracy: 0.6004 - form_output_loss: 0.6693 - loss: 1.2023 - val_exercise_output_accuracy: 0.5575 - val_exercise_output_loss: 1.4642 - val_form_output_accuracy: 0.6125 - val_form_output_loss: 0.6712 - val_loss: 1.2302 - learning_rate: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5346 - exercise_output_loss: 1.3950 - form_output_accuracy: 0.5989 - form_output_loss: 0.6684 - loss: 1.1770\n",
      "Epoch 45: val_loss did not improve from 1.21587\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5343 - exercise_output_loss: 1.3958 - form_output_accuracy: 0.5995 - form_output_loss: 0.6681 - loss: 1.1774 - val_exercise_output_accuracy: 0.5486 - val_exercise_output_loss: 1.4932 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6722 - val_loss: 1.2467 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5155 - exercise_output_loss: 1.4335 - form_output_accuracy: 0.6125 - form_output_loss: 0.6649 - loss: 1.2029\n",
      "Epoch 46: val_loss did not improve from 1.21587\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5162 - exercise_output_loss: 1.4331 - form_output_accuracy: 0.6123 - form_output_loss: 0.6651 - loss: 1.2027 - val_exercise_output_accuracy: 0.5729 - val_exercise_output_loss: 1.4629 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6733 - val_loss: 1.2274 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5609 - exercise_output_loss: 1.3649 - form_output_accuracy: 0.6079 - form_output_loss: 0.6668 - loss: 1.1554\n",
      "Epoch 47: val_loss did not improve from 1.21587\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5584 - exercise_output_loss: 1.3688 - form_output_accuracy: 0.6075 - form_output_loss: 0.6667 - loss: 1.1582 - val_exercise_output_accuracy: 0.5473 - val_exercise_output_loss: 1.4914 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6731 - val_loss: 1.2446 - learning_rate: 2.5000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5544 - exercise_output_loss: 1.4009 - form_output_accuracy: 0.6041 - form_output_loss: 0.6642 - loss: 1.1799\n",
      "Epoch 48: val_loss improved from 1.21587 to 1.20096, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5543 - exercise_output_loss: 1.3989 - form_output_accuracy: 0.6042 - form_output_loss: 0.6645 - loss: 1.1786 - val_exercise_output_accuracy: 0.5742 - val_exercise_output_loss: 1.4276 - val_form_output_accuracy: 0.5985 - val_form_output_loss: 0.6724 - val_loss: 1.2010 - learning_rate: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5325 - exercise_output_loss: 1.4112 - form_output_accuracy: 0.5990 - form_output_loss: 0.6702 - loss: 1.1889\n",
      "Epoch 49: val_loss did not improve from 1.20096\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5330 - exercise_output_loss: 1.4100 - form_output_accuracy: 0.5995 - form_output_loss: 0.6702 - loss: 1.1880 - val_exercise_output_accuracy: 0.5793 - val_exercise_output_loss: 1.4455 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6727 - val_loss: 1.2126 - learning_rate: 2.5000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5495 - exercise_output_loss: 1.3651 - form_output_accuracy: 0.6007 - form_output_loss: 0.6745 - loss: 1.1579\n",
      "Epoch 50: val_loss improved from 1.20096 to 1.19561, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5496 - exercise_output_loss: 1.3655 - form_output_accuracy: 0.6016 - form_output_loss: 0.6740 - loss: 1.1581 - val_exercise_output_accuracy: 0.5908 - val_exercise_output_loss: 1.4188 - val_form_output_accuracy: 0.6100 - val_form_output_loss: 0.6716 - val_loss: 1.1956 - learning_rate: 2.5000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5561 - exercise_output_loss: 1.3488 - form_output_accuracy: 0.6211 - form_output_loss: 0.6627 - loss: 1.1430\n",
      "Epoch 51: val_loss improved from 1.19561 to 1.18873, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5549 - exercise_output_loss: 1.3498 - form_output_accuracy: 0.6208 - form_output_loss: 0.6630 - loss: 1.1437 - val_exercise_output_accuracy: 0.5857 - val_exercise_output_loss: 1.4080 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6717 - val_loss: 1.1887 - learning_rate: 2.5000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5412 - exercise_output_loss: 1.3630 - form_output_accuracy: 0.6084 - form_output_loss: 0.6657 - loss: 1.1538\n",
      "Epoch 52: val_loss did not improve from 1.18873\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5414 - exercise_output_loss: 1.3642 - form_output_accuracy: 0.6083 - form_output_loss: 0.6657 - loss: 1.1546 - val_exercise_output_accuracy: 0.5460 - val_exercise_output_loss: 1.4914 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6729 - val_loss: 1.2432 - learning_rate: 2.5000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5500 - exercise_output_loss: 1.3580 - form_output_accuracy: 0.6001 - form_output_loss: 0.6703 - loss: 1.1517\n",
      "Epoch 53: val_loss did not improve from 1.18873\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5497 - exercise_output_loss: 1.3576 - form_output_accuracy: 0.6007 - form_output_loss: 0.6701 - loss: 1.1513 - val_exercise_output_accuracy: 0.5665 - val_exercise_output_loss: 1.4193 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6726 - val_loss: 1.1964 - learning_rate: 2.5000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5596 - exercise_output_loss: 1.3267 - form_output_accuracy: 0.6035 - form_output_loss: 0.6652 - loss: 1.1282\n",
      "Epoch 54: val_loss did not improve from 1.18873\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5589 - exercise_output_loss: 1.3294 - form_output_accuracy: 0.6044 - form_output_loss: 0.6652 - loss: 1.1301 - val_exercise_output_accuracy: 0.5818 - val_exercise_output_loss: 1.4105 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6739 - val_loss: 1.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5581 - exercise_output_loss: 1.3505 - form_output_accuracy: 0.6089 - form_output_loss: 0.6682 - loss: 1.1458\n",
      "Epoch 55: val_loss did not improve from 1.18873\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5585 - exercise_output_loss: 1.3492 - form_output_accuracy: 0.6098 - form_output_loss: 0.6681 - loss: 1.1449 - val_exercise_output_accuracy: 0.5793 - val_exercise_output_loss: 1.4188 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6724 - val_loss: 1.1952 - learning_rate: 2.5000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5426 - exercise_output_loss: 1.3669 - form_output_accuracy: 0.6010 - form_output_loss: 0.6690 - loss: 1.1575\n",
      "Epoch 56: val_loss did not improve from 1.18873\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5443 - exercise_output_loss: 1.3646 - form_output_accuracy: 0.6017 - form_output_loss: 0.6687 - loss: 1.1558 - val_exercise_output_accuracy: 0.5895 - val_exercise_output_loss: 1.4209 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6729 - val_loss: 1.1976 - learning_rate: 2.5000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5863 - exercise_output_loss: 1.2663 - form_output_accuracy: 0.6144 - form_output_loss: 0.6605 - loss: 1.0845\n",
      "Epoch 57: val_loss did not improve from 1.18873\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5858 - exercise_output_loss: 1.2676 - form_output_accuracy: 0.6143 - form_output_loss: 0.6606 - loss: 1.0855 - val_exercise_output_accuracy: 0.5857 - val_exercise_output_loss: 1.4089 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6736 - val_loss: 1.1903 - learning_rate: 1.2500e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5433 - exercise_output_loss: 1.3422 - form_output_accuracy: 0.6016 - form_output_loss: 0.6642 - loss: 1.1388\n",
      "Epoch 58: val_loss improved from 1.18873 to 1.17804, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5438 - exercise_output_loss: 1.3415 - form_output_accuracy: 0.6020 - form_output_loss: 0.6641 - loss: 1.1383 - val_exercise_output_accuracy: 0.5921 - val_exercise_output_loss: 1.3922 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6732 - val_loss: 1.1780 - learning_rate: 1.2500e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5737 - exercise_output_loss: 1.3108 - form_output_accuracy: 0.6108 - form_output_loss: 0.6631 - loss: 1.1165\n",
      "Epoch 59: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5726 - exercise_output_loss: 1.3112 - form_output_accuracy: 0.6106 - form_output_loss: 0.6637 - loss: 1.1169 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.4044 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6737 - val_loss: 1.1863 - learning_rate: 1.2500e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5743 - exercise_output_loss: 1.2860 - form_output_accuracy: 0.6219 - form_output_loss: 0.6571 - loss: 1.0973\n",
      "Epoch 60: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5737 - exercise_output_loss: 1.2889 - form_output_accuracy: 0.6213 - form_output_loss: 0.6577 - loss: 1.0996 - val_exercise_output_accuracy: 0.5882 - val_exercise_output_loss: 1.3998 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6745 - val_loss: 1.1835 - learning_rate: 1.2500e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5833 - exercise_output_loss: 1.2777 - form_output_accuracy: 0.5813 - form_output_loss: 0.6765 - loss: 1.0973\n",
      "Epoch 61: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5818 - exercise_output_loss: 1.2810 - form_output_accuracy: 0.5829 - form_output_loss: 0.6757 - loss: 1.0994 - val_exercise_output_accuracy: 0.5882 - val_exercise_output_loss: 1.4019 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6745 - val_loss: 1.1863 - learning_rate: 1.2500e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5526 - exercise_output_loss: 1.3199 - form_output_accuracy: 0.6311 - form_output_loss: 0.6522 - loss: 1.1196\n",
      "Epoch 62: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5533 - exercise_output_loss: 1.3194 - form_output_accuracy: 0.6297 - form_output_loss: 0.6531 - loss: 1.1195 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3989 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6746 - val_loss: 1.1826 - learning_rate: 1.2500e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5650 - exercise_output_loss: 1.3137 - form_output_accuracy: 0.5921 - form_output_loss: 0.6690 - loss: 1.1203\n",
      "Epoch 63: val_loss did not improve from 1.17804\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5654 - exercise_output_loss: 1.3128 - form_output_accuracy: 0.5929 - form_output_loss: 0.6687 - loss: 1.1196 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3993 - val_form_output_accuracy: 0.5997 - val_form_output_loss: 0.6731 - val_loss: 1.1828 - learning_rate: 1.2500e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5664 - exercise_output_loss: 1.3259 - form_output_accuracy: 0.6125 - form_output_loss: 0.6625 - loss: 1.1269\n",
      "Epoch 64: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5667 - exercise_output_loss: 1.3258 - form_output_accuracy: 0.6121 - form_output_loss: 0.6626 - loss: 1.1268 - val_exercise_output_accuracy: 0.5946 - val_exercise_output_loss: 1.3917 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6731 - val_loss: 1.1781 - learning_rate: 6.2500e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5492 - exercise_output_loss: 1.3608 - form_output_accuracy: 0.6114 - form_output_loss: 0.6654 - loss: 1.1522\n",
      "Epoch 65: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5522 - exercise_output_loss: 1.3543 - form_output_accuracy: 0.6119 - form_output_loss: 0.6652 - loss: 1.1476 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3960 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6733 - val_loss: 1.1808 - learning_rate: 6.2500e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5535 - exercise_output_loss: 1.3320 - form_output_accuracy: 0.6192 - form_output_loss: 0.6619 - loss: 1.1309\n",
      "Epoch 66: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5543 - exercise_output_loss: 1.3297 - form_output_accuracy: 0.6183 - form_output_loss: 0.6622 - loss: 1.1294 - val_exercise_output_accuracy: 0.5934 - val_exercise_output_loss: 1.3974 - val_form_output_accuracy: 0.6023 - val_form_output_loss: 0.6735 - val_loss: 1.1816 - learning_rate: 6.2500e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5732 - exercise_output_loss: 1.3135 - form_output_accuracy: 0.6150 - form_output_loss: 0.6588 - loss: 1.1171\n",
      "Epoch 67: val_loss did not improve from 1.17804\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5724 - exercise_output_loss: 1.3140 - form_output_accuracy: 0.6145 - form_output_loss: 0.6592 - loss: 1.1175 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3916 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6731 - val_loss: 1.1781 - learning_rate: 6.2500e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.6026 - exercise_output_loss: 1.2539 - form_output_accuracy: 0.6147 - form_output_loss: 0.6689 - loss: 1.0784\n",
      "Epoch 68: val_loss improved from 1.17804 to 1.17802, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.6017 - exercise_output_loss: 1.2548 - form_output_accuracy: 0.6147 - form_output_loss: 0.6688 - loss: 1.0790 - val_exercise_output_accuracy: 0.5921 - val_exercise_output_loss: 1.3918 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6735 - val_loss: 1.1780 - learning_rate: 6.2500e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5680 - exercise_output_loss: 1.3245 - form_output_accuracy: 0.6065 - form_output_loss: 0.6654 - loss: 1.1267\n",
      "Epoch 69: val_loss improved from 1.17802 to 1.17778, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5690 - exercise_output_loss: 1.3221 - form_output_accuracy: 0.6071 - form_output_loss: 0.6651 - loss: 1.1250 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3911 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6736 - val_loss: 1.1778 - learning_rate: 3.1250e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m97/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5743 - exercise_output_loss: 1.2833 - form_output_accuracy: 0.6047 - form_output_loss: 0.6686 - loss: 1.0989\n",
      "Epoch 70: val_loss did not improve from 1.17778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5742 - exercise_output_loss: 1.2835 - form_output_accuracy: 0.6048 - form_output_loss: 0.6685 - loss: 1.0990 - val_exercise_output_accuracy: 0.5921 - val_exercise_output_loss: 1.3944 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6739 - val_loss: 1.1798 - learning_rate: 3.1250e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m87/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5570 - exercise_output_loss: 1.3183 - form_output_accuracy: 0.6264 - form_output_loss: 0.6596 - loss: 1.1207\n",
      "Epoch 71: val_loss did not improve from 1.17778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5593 - exercise_output_loss: 1.3159 - form_output_accuracy: 0.6247 - form_output_loss: 0.6598 - loss: 1.1190 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3918 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6738 - val_loss: 1.1781 - learning_rate: 3.1250e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5717 - exercise_output_loss: 1.2632 - form_output_accuracy: 0.6078 - form_output_loss: 0.6659 - loss: 1.0840\n",
      "Epoch 72: val_loss did not improve from 1.17778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5717 - exercise_output_loss: 1.2634 - form_output_accuracy: 0.6078 - form_output_loss: 0.6659 - loss: 1.0841 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3919 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6734 - val_loss: 1.1782 - learning_rate: 3.1250e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5668 - exercise_output_loss: 1.3331 - form_output_accuracy: 0.6302 - form_output_loss: 0.6541 - loss: 1.1294\n",
      "Epoch 73: val_loss did not improve from 1.17778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5673 - exercise_output_loss: 1.3299 - form_output_accuracy: 0.6284 - form_output_loss: 0.6549 - loss: 1.1274 - val_exercise_output_accuracy: 0.5997 - val_exercise_output_loss: 1.3929 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6733 - val_loss: 1.1790 - learning_rate: 3.1250e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5845 - exercise_output_loss: 1.2803 - form_output_accuracy: 0.6339 - form_output_loss: 0.6513 - loss: 1.0916\n",
      "Epoch 74: val_loss did not improve from 1.17778\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5837 - exercise_output_loss: 1.2822 - form_output_accuracy: 0.6312 - form_output_loss: 0.6526 - loss: 1.0934 - val_exercise_output_accuracy: 0.5997 - val_exercise_output_loss: 1.3934 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6736 - val_loss: 1.1796 - learning_rate: 3.1250e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m86/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5799 - exercise_output_loss: 1.2480 - form_output_accuracy: 0.6303 - form_output_loss: 0.6536 - loss: 1.0696\n",
      "Epoch 75: val_loss did not improve from 1.17778\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5799 - exercise_output_loss: 1.2516 - form_output_accuracy: 0.6285 - form_output_loss: 0.6549 - loss: 1.0725 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3931 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6734 - val_loss: 1.1793 - learning_rate: 1.5625e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5685 - exercise_output_loss: 1.3093 - form_output_accuracy: 0.6148 - form_output_loss: 0.6626 - loss: 1.1153\n",
      "Epoch 76: val_loss improved from 1.17778 to 1.17750, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5683 - exercise_output_loss: 1.3088 - form_output_accuracy: 0.6147 - form_output_loss: 0.6627 - loss: 1.1149 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3905 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6732 - val_loss: 1.1775 - learning_rate: 1.5625e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - exercise_output_accuracy: 0.5813 - exercise_output_loss: 1.2770 - form_output_accuracy: 0.6030 - form_output_loss: 0.6687 - loss: 1.0945\n",
      "Epoch 77: val_loss improved from 1.17750 to 1.17708, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5810 - exercise_output_loss: 1.2774 - form_output_accuracy: 0.6032 - form_output_loss: 0.6685 - loss: 1.0947 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3902 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6731 - val_loss: 1.1771 - learning_rate: 1.5625e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5791 - exercise_output_loss: 1.3032 - form_output_accuracy: 0.6112 - form_output_loss: 0.6613 - loss: 1.1106\n",
      "Epoch 78: val_loss did not improve from 1.17708\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5792 - exercise_output_loss: 1.3025 - form_output_accuracy: 0.6111 - form_output_loss: 0.6614 - loss: 1.1101 - val_exercise_output_accuracy: 0.5934 - val_exercise_output_loss: 1.3908 - val_form_output_accuracy: 0.6049 - val_form_output_loss: 0.6730 - val_loss: 1.1773 - learning_rate: 1.5625e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5722 - exercise_output_loss: 1.2804 - form_output_accuracy: 0.6211 - form_output_loss: 0.6627 - loss: 1.0951\n",
      "Epoch 79: val_loss did not improve from 1.17708\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5721 - exercise_output_loss: 1.2833 - form_output_accuracy: 0.6203 - form_output_loss: 0.6629 - loss: 1.0972 - val_exercise_output_accuracy: 0.5908 - val_exercise_output_loss: 1.3908 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6731 - val_loss: 1.1772 - learning_rate: 1.5625e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5884 - exercise_output_loss: 1.2742 - form_output_accuracy: 0.6302 - form_output_loss: 0.6546 - loss: 1.0883\n",
      "Epoch 80: val_loss did not improve from 1.17708\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5879 - exercise_output_loss: 1.2748 - form_output_accuracy: 0.6285 - form_output_loss: 0.6553 - loss: 1.0890 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3912 - val_form_output_accuracy: 0.6036 - val_form_output_loss: 0.6732 - val_loss: 1.1776 - learning_rate: 1.5625e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5759 - exercise_output_loss: 1.2690 - form_output_accuracy: 0.6317 - form_output_loss: 0.6572 - loss: 1.0854\n",
      "Epoch 81: val_loss improved from 1.17708 to 1.17662, saving model to gym_form_detector_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - exercise_output_accuracy: 0.5758 - exercise_output_loss: 1.2694 - form_output_accuracy: 0.6301 - form_output_loss: 0.6578 - loss: 1.0859 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3899 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6731 - val_loss: 1.1766 - learning_rate: 1.5625e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5641 - exercise_output_loss: 1.3097 - form_output_accuracy: 0.6240 - form_output_loss: 0.6539 - loss: 1.1130\n",
      "Epoch 82: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5646 - exercise_output_loss: 1.3084 - form_output_accuracy: 0.6229 - form_output_loss: 0.6547 - loss: 1.1123 - val_exercise_output_accuracy: 0.5934 - val_exercise_output_loss: 1.3915 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6731 - val_loss: 1.1779 - learning_rate: 1.5625e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5732 - exercise_output_loss: 1.2883 - form_output_accuracy: 0.6172 - form_output_loss: 0.6580 - loss: 1.0992\n",
      "Epoch 83: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5726 - exercise_output_loss: 1.2894 - form_output_accuracy: 0.6168 - form_output_loss: 0.6585 - loss: 1.1001 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3911 - val_form_output_accuracy: 0.6074 - val_form_output_loss: 0.6731 - val_loss: 1.1775 - learning_rate: 1.5625e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5667 - exercise_output_loss: 1.3000 - form_output_accuracy: 0.6126 - form_output_loss: 0.6602 - loss: 1.1081\n",
      "Epoch 84: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5666 - exercise_output_loss: 1.3000 - form_output_accuracy: 0.6124 - form_output_loss: 0.6603 - loss: 1.1081 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3908 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6729 - val_loss: 1.1773 - learning_rate: 1.5625e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m92/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5535 - exercise_output_loss: 1.3131 - form_output_accuracy: 0.6319 - form_output_loss: 0.6535 - loss: 1.1152\n",
      "Epoch 85: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5544 - exercise_output_loss: 1.3120 - form_output_accuracy: 0.6307 - form_output_loss: 0.6542 - loss: 1.1146 - val_exercise_output_accuracy: 0.5934 - val_exercise_output_loss: 1.3925 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6732 - val_loss: 1.1785 - learning_rate: 1.5625e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5831 - exercise_output_loss: 1.2882 - form_output_accuracy: 0.6149 - form_output_loss: 0.6613 - loss: 1.1001\n",
      "Epoch 86: val_loss did not improve from 1.17662\n",
      "\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5830 - exercise_output_loss: 1.2884 - form_output_accuracy: 0.6149 - form_output_loss: 0.6613 - loss: 1.1003 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3922 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6731 - val_loss: 1.1785 - learning_rate: 1.5625e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m89/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5773 - exercise_output_loss: 1.2776 - form_output_accuracy: 0.6101 - form_output_loss: 0.6613 - loss: 1.0927\n",
      "Epoch 87: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5777 - exercise_output_loss: 1.2775 - form_output_accuracy: 0.6101 - form_output_loss: 0.6614 - loss: 1.0927 - val_exercise_output_accuracy: 0.5946 - val_exercise_output_loss: 1.3926 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6732 - val_loss: 1.1786 - learning_rate: 7.8125e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5697 - exercise_output_loss: 1.3109 - form_output_accuracy: 0.6181 - form_output_loss: 0.6598 - loss: 1.1156\n",
      "Epoch 88: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5701 - exercise_output_loss: 1.3097 - form_output_accuracy: 0.6180 - form_output_loss: 0.6599 - loss: 1.1147 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3921 - val_form_output_accuracy: 0.6074 - val_form_output_loss: 0.6732 - val_loss: 1.1783 - learning_rate: 7.8125e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m95/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5623 - exercise_output_loss: 1.3385 - form_output_accuracy: 0.6063 - form_output_loss: 0.6670 - loss: 1.1371\n",
      "Epoch 89: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5626 - exercise_output_loss: 1.3369 - form_output_accuracy: 0.6068 - form_output_loss: 0.6669 - loss: 1.1359 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3910 - val_form_output_accuracy: 0.6074 - val_form_output_loss: 0.6733 - val_loss: 1.1776 - learning_rate: 7.8125e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m94/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5840 - exercise_output_loss: 1.2760 - form_output_accuracy: 0.5998 - form_output_loss: 0.6762 - loss: 1.0960\n",
      "Epoch 90: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5838 - exercise_output_loss: 1.2759 - form_output_accuracy: 0.6006 - form_output_loss: 0.6757 - loss: 1.0958 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3909 - val_form_output_accuracy: 0.6061 - val_form_output_loss: 0.6733 - val_loss: 1.1775 - learning_rate: 7.8125e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5873 - exercise_output_loss: 1.2594 - form_output_accuracy: 0.6071 - form_output_loss: 0.6663 - loss: 1.0815\n",
      "Epoch 91: val_loss did not improve from 1.17662\n",
      "\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5871 - exercise_output_loss: 1.2600 - form_output_accuracy: 0.6072 - form_output_loss: 0.6662 - loss: 1.0819 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3911 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6732 - val_loss: 1.1775 - learning_rate: 7.8125e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m96/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5755 - exercise_output_loss: 1.2727 - form_output_accuracy: 0.6254 - form_output_loss: 0.6554 - loss: 1.0875\n",
      "Epoch 92: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5756 - exercise_output_loss: 1.2728 - form_output_accuracy: 0.6252 - form_output_loss: 0.6556 - loss: 1.0876 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3921 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6732 - val_loss: 1.1783 - learning_rate: 3.9063e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m88/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5645 - exercise_output_loss: 1.3059 - form_output_accuracy: 0.6202 - form_output_loss: 0.6613 - loss: 1.1125\n",
      "Epoch 93: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5644 - exercise_output_loss: 1.3062 - form_output_accuracy: 0.6194 - form_output_loss: 0.6616 - loss: 1.1128 - val_exercise_output_accuracy: 0.5946 - val_exercise_output_loss: 1.3921 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6731 - val_loss: 1.1782 - learning_rate: 3.9063e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5744 - exercise_output_loss: 1.3085 - form_output_accuracy: 0.6203 - form_output_loss: 0.6607 - loss: 1.1142\n",
      "Epoch 94: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5750 - exercise_output_loss: 1.3069 - form_output_accuracy: 0.6198 - form_output_loss: 0.6609 - loss: 1.1131 - val_exercise_output_accuracy: 0.5985 - val_exercise_output_loss: 1.3913 - val_form_output_accuracy: 0.6074 - val_form_output_loss: 0.6732 - val_loss: 1.1777 - learning_rate: 3.9063e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5735 - exercise_output_loss: 1.2620 - form_output_accuracy: 0.6130 - form_output_loss: 0.6659 - loss: 1.0832\n",
      "Epoch 95: val_loss did not improve from 1.17662\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5735 - exercise_output_loss: 1.2622 - form_output_accuracy: 0.6130 - form_output_loss: 0.6659 - loss: 1.0833 - val_exercise_output_accuracy: 0.5972 - val_exercise_output_loss: 1.3919 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6731 - val_loss: 1.1781 - learning_rate: 3.9063e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - exercise_output_accuracy: 0.5759 - exercise_output_loss: 1.2672 - form_output_accuracy: 0.6032 - form_output_loss: 0.6677 - loss: 1.0874\n",
      "Epoch 96: val_loss did not improve from 1.17662\n",
      "\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - exercise_output_accuracy: 0.5765 - exercise_output_loss: 1.2674 - form_output_accuracy: 0.6042 - form_output_loss: 0.6674 - loss: 1.0874 - val_exercise_output_accuracy: 0.5959 - val_exercise_output_loss: 1.3927 - val_form_output_accuracy: 0.6087 - val_form_output_loss: 0.6731 - val_loss: 1.1787 - learning_rate: 3.9063e-06\n",
      "Epoch 96: early stopping\n",
      "Restoring model weights from the end of the best epoch: 81.\n",
      "✅ Modelo treinado e salvo em gym_form_detector_model.h5\n",
      "\n",
      "✅ Webcam ativada. Pressione 'Q' para sair da janela de visualização.\n",
      "UTC: 08/07/2025 22:06:10 (UTC)\n",
      "Brasília: 08/07/2025 16:06:10 (UTC-3)\n",
      "\n",
      "⚠️ A performance pode ser limitada pela sua CPU. Para melhor experiência, considere GPUs dedicadas.\n"
     ]
    }
   ],
   "source": [
    "# 1. Instalação das Dependências\n",
    "# Garantir que todas as bibliotecas necessárias estejam instaladas.\n",
    "# Adicionamos 'mediapipe' que é essencial para a estimativa de pose.\n",
    "# O pacote 'tensorflow' por padrão instala a versão otimizada para CPU.\n",
    "\n",
    "# Removido 'tensorflow-gpu' e 'gdown' já que estamos em CPU e o dataset é baixado com kaggle cli\n",
    "# A linha abaixo deve ser executada uma vez no terminal ou na célula de notebook para instalar/atualizar\n",
    "# !pip install tensorflow numpy pandas matplotlib requests scikit-learn imbalanced-learn tqdm kaggle mediapipe opencv-python --upgrade\n",
    "# !pip install --upgrade h5py Keras\n",
    "\n",
    "# 2. Configurações Iniciais e Verificação de Hardware\n",
    "# Importações essenciais para o projeto.\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import sys # Importado para sys.exit()\n",
    "\n",
    "# Importar MediaPipe para estimativa de pose\n",
    "import mediapipe as mp\n",
    "\n",
    "# Configuração de CPU\n",
    "# Em um ambiente com apenas CPU, o TensorFlow irá automaticamente utilizar o processador.\n",
    "print(\"ℹ️ Nenhuma GPU NVIDIA detectada ou configurada. Todas as operações de Machine Learning utilizarão a CPU.\")\n",
    "\n",
    "# Configurar credenciais do Kaggle\n",
    "# Isso é crucial para baixar o dataset de exercícios automaticamente.\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Você pode obter isso no seu perfil do Kaggle, na seção \"API\".\n",
    "kaggle_creds = {\n",
    "    \"username\": \"leobiondi\", # Substitua pelo seu username do Kaggle\n",
    "    \"key\": \"4bfa4af56e33c61cc4852d2e5f626721\" # Substitua pela sua chave API do Kaggle\n",
    "}\n",
    "\n",
    "# Salvar credenciais no arquivo kaggle.json\n",
    "with open(kaggle_dir / 'kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "\n",
    "# Adicionar ao PATH para que o comando 'kaggle' possa ser executado\n",
    "# Isso é importante para que o subprocess.run possa encontrar o comando kaggle.\n",
    "os.environ['PATH'] += os.pathsep + str(Path.home() / '.local' / 'bin')\n",
    "\n",
    "print(\"✅ Ambiente configurado. Credenciais do Kaggle salvas e ambiente preparado para CPU.\")\n",
    "\n",
    "\n",
    "# 3. Definição de Funções Auxiliares para Extração de Features (MediaPipe) e Geração de Erros\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detecção de landmarks.\n",
    "# Static_image_mode=False é para processamento de vídeo, True para imagens estáticas.\n",
    "# Min_detection_confidence e min_tracking_confidence são limiares de confiança.\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles # Corrigido aqui, estava mp.solutions.drawing_utils novamente\n",
    "\n",
    "# Função para calcular o ângulo entre três pontos (landmarks)\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calcula o ângulo em graus entre três pontos (landmarks).\n",
    "    O ponto 'b' é o vértice do ângulo.\n",
    "    Args:\n",
    "        a (list/tuple): Coordenadas (x, y) ou (x, y, z) do primeiro ponto.\n",
    "        b (list/tuple): Coordenadas (x, y) ou (x, y, z) do ponto central (vértice).\n",
    "        c (list/tuple): Coordenadas (x, y) ou (x, y, z) do terceiro ponto.\n",
    "    Returns:\n",
    "        float: O ângulo em graus.\n",
    "    \"\"\"\n",
    "    a = np.array(a) # Primeiro ponto\n",
    "    b = np.array(b) # Ponto do vértice\n",
    "    c = np.array(c) # Terceiro ponto\n",
    "\n",
    "    # Vetores formados pelos pontos\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # Produto escalar e normas dos vetores\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    # Garantir que o valor esteja dentro do domínio de arccos para evitar erros de floating point\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Função para extrair features de pose de uma imagem (ângulos e distâncias)\n",
    "def extract_pose_features(image_path, pose_detector):\n",
    "    \"\"\"\n",
    "    Extrai um vetor de features de pose (ângulos chave do corpo) de uma imagem.\n",
    "    Args:\n",
    "        image_path (str): Caminho para a imagem.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado.\n",
    "    Returns:\n",
    "        numpy.ndarray or None: Um vetor numpy de features (ângulos), ou None se nenhuma pose for detectada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lendo a imagem\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # print(f\"⚠️ Não foi possível carregar a imagem: {image_path}\") # Comentado para evitar poluir o log\n",
    "            return None\n",
    "\n",
    "        # Convertendo a imagem de BGR para RGB (MediaPipe espera RGB)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Processando a imagem para estimativa de pose\n",
    "        results = pose_detector.process(image_rgb)\n",
    "\n",
    "        # Se houver landmarks de pose detectadas\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Mapeamento das landmarks do MediaPipe para fácil acesso\n",
    "            # Para o `mp_pose.PoseLandmark` o `.value` é necessário para acessar o índice correto\n",
    "            \n",
    "            # Ângulos dos cotovelos (ombro, cotovelo, punho)\n",
    "            left_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            )\n",
    "            right_elbow_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            )\n",
    "\n",
    "            # Ângulos dos ombros (quadril, ombro, cotovelo)\n",
    "            left_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            )\n",
    "            right_shoulder_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            )\n",
    "            \n",
    "            # Ângulos dos joelhos (quadril, joelho, tornozelo)\n",
    "            left_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            )\n",
    "            right_knee_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            )\n",
    "\n",
    "            # Ângulos dos quadris (ombro, quadril, joelho) - essencial para postura de tronco\n",
    "            left_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "            )\n",
    "            right_hip_angle = calculate_angle(\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "            )\n",
    "            \n",
    "            # Coletar todas as features em um vetor\n",
    "            features = [\n",
    "                left_elbow_angle, right_elbow_angle,\n",
    "                left_shoulder_angle, right_shoulder_angle,\n",
    "                left_knee_angle, right_knee_angle,\n",
    "                left_hip_angle, right_hip_angle\n",
    "            ]\n",
    "            \n",
    "            return np.array(features)\n",
    "        else:\n",
    "            return None # Nenhuma pose detectada\n",
    "    except Exception as e:\n",
    "        # print(f\"❌ Erro ao extrair features de {image_path}: {e}\") # Comentado para evitar poluir o log\n",
    "        return None\n",
    "\n",
    "# Nova função para gerar formas incorretas sinteticamente\n",
    "def synthesize_incorrect_form(original_features, exercise_type_str):\n",
    "    \"\"\"\n",
    "    Aplica uma perturbação sintética a uma cópia do array de features original\n",
    "    para simular uma forma incorreta comum para o tipo de exercício especificado.\n",
    "\n",
    "    Args:\n",
    "        original_features (np.ndarray): O array de features (ângulos) da pose original (correta).\n",
    "        exercise_type_str (str): O nome do exercício (ex: 'squat', 'deadlift', 'bicep curl').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: O array de features modificado para simular a forma incorreta.\n",
    "    \"\"\"\n",
    "    modified_features = original_features.copy()\n",
    "    \n",
    "    # Magnitude da perturbação angular (em graus)\n",
    "    deviation_magnitude = np.random.uniform(15, 35) # Desvio de 15 a 35 graus\n",
    "\n",
    "    if exercise_type_str == 'squat':\n",
    "        # Erro comum: Não ir fundo o suficiente (joelhos não dobram o bastante)\n",
    "        # Aumentar o ângulo do joelho para simular falta de profundidade\n",
    "        modified_features[4] = np.clip(modified_features[4] + deviation_magnitude, 0, 180) # Left knee\n",
    "        modified_features[5] = np.clip(modified_features[5] + deviation_magnitude, 0, 180) # Right knee\n",
    "        \n",
    "    elif exercise_type_str == 'deadlift':\n",
    "        # Erro comum: Costas arredondadas (quadril muito baixo ou muito reto)\n",
    "        # Diminuir o ângulo do quadril para simular um tronco menos inclinado para frente ou mais arredondado\n",
    "        modified_features[6] = np.clip(modified_features[6] - deviation_magnitude, 0, 180) # Left hip\n",
    "        modified_features[7] = np.clip(modified_features[7] - deviation_magnitude, 0, 180) # Right hip\n",
    "        \n",
    "    elif exercise_type_str == 'bicep curl':\n",
    "        # Erro comum: Extensão incompleta (não esticar totalmente o braço na parte inferior)\n",
    "        # Manter o ângulo do cotovelo maior do que deveria estar na extensão máxima\n",
    "        modified_features[0] = np.clip(modified_features[0] - deviation_magnitude, 0, 180) # Left elbow (make it less straight)\n",
    "        modified_features[1] = np.clip(modified_features[1] - deviation_magnitude, 0, 180) # Right elbow (make it less straight)\n",
    "        \n",
    "    # Para outros exercícios não especificados, o desvio não será aplicado.\n",
    "    return modified_features\n",
    "\n",
    "print(\"✅ Funções de extração de pose, cálculo de ângulos e geração de erros sintéticos prontas.\")\n",
    "\n",
    "# 4. Baixar e Pré-processar o Dataset de Exercícios (Com Geração de Erros)\n",
    "\n",
    "# Definir o nome do dataset do Kaggle para exercícios\n",
    "DATASET_SLUG = \"hasyimabdillah/workoutexercises-images\"\n",
    "DATASET_NAME = \"workout-exercises-images\" # Nome da pasta onde será extraído\n",
    "DATASET_PATH = Path(DATASET_NAME) # Caminho local\n",
    "\n",
    "# Função para baixar e extrair o dataset do Kaggle\n",
    "def download_exercise_dataset(dataset_slug, target_path):\n",
    "    if not target_path.exists():\n",
    "        print(f\"⬇️ Baixando {dataset_slug} do Kaggle...\")\n",
    "        try:\n",
    "            # Comando Kaggle para baixar e descompactar\n",
    "            # O subprocess.run precisa que o comando 'kaggle' esteja acessível no PATH do ambiente\n",
    "            subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", dataset_slug, \"-p\", str(target_path.parent), \"--unzip\"], check=True)\n",
    "            print(f\"✅ {dataset_slug} baixado e extraído para {target_path.parent}!\")\n",
    "            \n",
    "            # O Kaggle pode baixar para uma pasta com o nome do slug, vamos renomear se necessário\n",
    "            # e garantir que a estrutura seja a esperada.\n",
    "            downloaded_dir = target_path.parent / dataset_slug.split('/')[-1]\n",
    "            if downloaded_dir.exists() and downloaded_dir != target_path:\n",
    "                print(f\"Renomeando '{downloaded_dir}' para '{target_path}'...\")\n",
    "                shutil.move(downloaded_dir, target_path)\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Erro ao baixar {dataset_slug} do Kaggle: {e}\")\n",
    "            print(\"Por favor, verifique se suas credenciais do Kaggle estão corretas e se você tem permissão para baixar o dataset.\")\n",
    "            print(\"Você pode precisar aceitar as regras do dataset no Kaggle primeiro: https://www.kaggle.com/datasets/hasyimabdillah/workoutexercises-images/code\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro inesperado ao baixar {dataset_slug}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"ℹ️ Dataset '{DATASET_NAME}' já existe em {target_path}. Pulando download.\")\n",
    "    return True\n",
    "\n",
    "# Baixar o dataset de exercícios\n",
    "dataset_downloaded = download_exercise_dataset(DATASET_SLUG, DATASET_PATH)\n",
    "\n",
    "if not dataset_downloaded:\n",
    "    print(\"❌ Não foi possível continuar sem o dataset. Encerrando o script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Listar os tipos de exercícios (subpastas dentro do dataset)\n",
    "exercise_types = [d.name for d in DATASET_PATH.iterdir() if d.is_dir()]\n",
    "print(f\"\\n✅ Tipos de exercícios encontrados no dataset: {exercise_types}\")\n",
    "\n",
    "# Processamento dos Dados de Exercícios: Extrair Features e Gerar Rótulos de Forma (Correta/Incorreta)\n",
    "\n",
    "# Listas para armazenar as features e os rótulos\n",
    "all_features = []\n",
    "all_exercise_labels = []\n",
    "all_form_labels = [] # 0 para correto, 1 para incorreto\n",
    "\n",
    "# Inicializar o objeto MediaPipe Pose para usar na extração de features\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_detector:\n",
    "    for exercise_type in exercise_types:\n",
    "        exercise_dir = DATASET_PATH / exercise_type\n",
    "        image_files = [f for f in exercise_dir.iterdir() if f.suffix.lower() in ('.jpg', '.jpeg', '.png')]\n",
    "        \n",
    "        print(f\"\\n⚙️ Processando {len(image_files)} imagens para o exercício: {exercise_type}\")\n",
    "        \n",
    "        # Amostragem para limitar o tamanho do dataset e acelerar o processamento durante o desenvolvimento.\n",
    "        # Reduzir este número acelerará o tempo de pré-processamento e treinamento.\n",
    "        SAMPLE_PER_EXERCISE = 200 # Processar até 200 imagens por tipo de exercício para um teste rápido\n",
    "        \n",
    "        if len(image_files) > SAMPLE_PER_EXERCISE:\n",
    "            # Use list() para garantir que a amostra seja uma lista de caminhos, não um array numpy\n",
    "            image_files_sampled = np.random.choice(image_files, SAMPLE_PER_EXERCISE, replace=False).tolist()\n",
    "            print(f\"   Amostrando {SAMPLE_PER_EXERCISE} imagens de {len(image_files)} disponíveis para {exercise_type}.\")\n",
    "        else:\n",
    "            image_files_sampled = image_files.tolist() # Se menos que a amostra, usar todas.\n",
    "\n",
    "        # Definir a proporção de exemplos \"incorretos\" que queremos gerar sinteticamente\n",
    "        # Isso ajuda a balancear o dataset para as classes \"correta\" e \"incorreta\"\n",
    "        synthetic_incorrect_ratio = 0.4 # Queremos que 40% das amostras processadas sejam sinteticamente incorretas\n",
    "        num_samples_to_make_incorrect = int(len(image_files_sampled) * synthetic_incorrect_ratio)\n",
    "        \n",
    "        # Selecionar aleatoriamente os índices das imagens onde aplicaremos o erro sintético\n",
    "        # Certifique-se de que não tentamos selecionar mais índices do que amostras disponíveis\n",
    "        indices_to_make_incorrect = np.random.choice(\n",
    "            len(image_files_sampled),\n",
    "            min(num_samples_to_make_incorrect, len(image_files_sampled)),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, img_file in enumerate(tqdm(image_files_sampled, desc=f\"   Extraindo features de {exercise_type}\")):\n",
    "            original_features = extract_pose_features(str(img_file), pose_detector)\n",
    "            \n",
    "            if original_features is not None:\n",
    "                # Decide if this sample should be made synthetically incorrect\n",
    "                if i in indices_to_make_incorrect:\n",
    "                    # Apply synthetic error and label as incorrect\n",
    "                    features_to_add = synthesize_incorrect_form(original_features, exercise_type)\n",
    "                    form_status = 1 # Incorreto\n",
    "                else:\n",
    "                    # Keep original features and label as correct\n",
    "                    features_to_add = original_features\n",
    "                    form_status = 0 # Correto\n",
    "\n",
    "                all_features.append(features_to_add)\n",
    "                all_exercise_labels.append(exercise_type)\n",
    "                all_form_labels.append(form_status)\n",
    "            else:\n",
    "                pass # Already prints error if image not loaded or pose not detected\n",
    "\n",
    "\n",
    "# Verificar se há dados para processar\n",
    "if not all_features:\n",
    "    print(\"\\n❌ Nenhuma feature foi extraída. Verifique seu dataset e a detecção de pose.\")\n",
    "    print(\"O script será encerrado pois não há dados para treinar o modelo.\")\n",
    "    sys.exit(1) # Encerrar o script\n",
    "\n",
    "# Converter listas para arrays NumPy\n",
    "X = np.array(all_features)\n",
    "y_exercise_raw = np.array(all_exercise_labels)\n",
    "y_form = np.array(all_form_labels)\n",
    "\n",
    "# Codificar rótulos de exercício e forma para formato numérico (one-hot encoding)\n",
    "# Para exercícios, precisamos de um LabelEncoder primeiro para mapear strings para inteiros.\n",
    "exercise_encoder = LabelEncoder()\n",
    "y_exercise_encoded = exercise_encoder.fit_transform(y_exercise_raw)\n",
    "y_exercise = to_categorical(y_exercise_encoded)\n",
    "\n",
    "# Para a forma, já temos 0/1, então podemos diretamente para one-hot se necessário.\n",
    "# Como é binário (0/1), to_categorical é útil para a função de perda.\n",
    "y_form = to_categorical(y_form, num_classes=2)\n",
    "\n",
    "print(f\"\\n✅ Total de {len(X)} amostras processadas.\")\n",
    "print(f\"   Shape das features (X): {X.shape}\")\n",
    "print(f\"   Shape dos rótulos de exercício (y_exercise): {y_exercise.shape}\")\n",
    "print(f\"   Shape dos rótulos de forma (y_form): {y_form.shape}\")\n",
    "print(f\"   Exercícios detectados: {exercise_encoder.classes_}\")\n",
    "\n",
    "# Salvar o LabelEncoder para uso posterior na inferência\n",
    "np.save('exercise_encoder.npy', exercise_encoder)\n",
    "\n",
    "# Normalizar as features (importante para redes neurais)\n",
    "# Usaremos MinMaxScaler para escalar as features entre 0 e 1.\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "np.save('feature_scaler.npy', feature_scaler)\n",
    "\n",
    "print(\"✅ Pré-processamento de dados de exercícios concluído e escalers salvos.\")\n",
    "\n",
    "# 5. Construção e Treinamento do Modelo de Detecção de Forma\n",
    "\n",
    "# Caminho para salvar o modelo\n",
    "MODEL_PATH = 'gym_form_detector_model.h5'\n",
    "\n",
    "# Verificar se um modelo pré-treinado já existe\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"ℹ️ Modelo pré-treinado encontrado. Carregando...\")\n",
    "    # O TensorFlow irá automaticamente carregar e usar a CPU se nenhuma GPU estiver disponível\n",
    "    model = load_model(MODEL_PATH)\n",
    "    \n",
    "    # Carregar scalers para consistência (necessário mesmo se o modelo já existe)\n",
    "    exercise_encoder = np.load('exercise_encoder.npy', allow_pickle=True).item()\n",
    "    feature_scaler = np.load('feature_scaler.npy', allow_pickle=True).item()\n",
    "    print(\"✅ Modelo pré-existente e scalers carregados.\")\n",
    "else:\n",
    "    print(\"⚙️ Modelo não encontrado. Iniciando construção e treinamento de novo modelo...\")\n",
    "\n",
    "    # Definir a arquitetura do modelo\n",
    "    def build_exercise_form_model(input_shape, num_exercise_classes):\n",
    "        # A entrada agora é um vetor de features (ângulos e distâncias)\n",
    "        input_layer = Input(shape=(input_shape,))\n",
    "        \n",
    "        # Camadas densas para processar as features\n",
    "        x = Dense(512, activation='relu')(input_layer)\n",
    "        x = BatchNormalization()(x) # Ajuda na estabilidade do treinamento\n",
    "        x = Dropout(0.4)(x) # Previne overfitting\n",
    "        \n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # Duas saídas para as duas tarefas de classificação\n",
    "        exercise_output = Dense(num_exercise_classes, activation='softmax', name='exercise_output')(x)\n",
    "        form_output = Dense(2, activation='softmax', name='form_output')(x) # 2 classes: correto/incorreto\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=[exercise_output, form_output])\n",
    "        return model\n",
    "\n",
    "    # Criar o modelo\n",
    "    num_features = X_scaled.shape[1] # Número de features extraídas\n",
    "    num_exercise_classes = y_exercise.shape[1] # Número de tipos de exercícios\n",
    "    model = build_exercise_form_model(num_features, num_exercise_classes)\n",
    "    \n",
    "    # Compilar o modelo\n",
    "    # Usamos 'categorical_crossentropy' para ambas as saídas, pois são problemas de classificação multi-classe.\n",
    "    # Os 'loss_weights' permitem balancear a importância de cada tarefa durante o treinamento.\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'exercise_output': 'categorical_crossentropy',\n",
    "            'form_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'exercise_output': 0.7, # Maior peso para classificar o exercício corretamente\n",
    "            'form_output': 0.3 # Menor peso, mas ainda importante para a forma\n",
    "        },\n",
    "        metrics={\n",
    "            'exercise_output': ['accuracy'],\n",
    "            'form_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks para um treinamento mais robusto\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        MODEL_PATH,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss', # Monitorar a perda de validação combinada\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5, # Reduz a LR pela metade\n",
    "        patience=5, # Se val_loss não melhorar por 5 épocas\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15, # Para o treinamento se val_loss não melhorar por 15 épocas\n",
    "        restore_best_weights=True, # Restaura os pesos do melhor modelo\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Dividir os dados em conjuntos de treino e validação\n",
    "    # Usamos o mesmo conjunto de features X_scaled para ambas as saídas.\n",
    "    X_train, X_val, y_exercise_train, y_exercise_val, y_form_train, y_form_val = train_test_split(\n",
    "        X_scaled, y_exercise, y_form, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    # Passamos os dicionários de labels para o fit.\n",
    "    # O treinamento ocorrerá na CPU.\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {'exercise_output': y_exercise_train, 'form_output': y_form_train},\n",
    "        epochs=100, # Um número razoável de épocas, EarlyStopping vai parar antes se convergir\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, {'exercise_output': y_exercise_val, 'form_output': y_form_val}),\n",
    "        callbacks=[checkpoint, reduce_lr, early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Modelo treinado e salvo em {MODEL_PATH}\")\n",
    "\n",
    "# 6. Detecção de Forma de Exercícios em Tempo Real\n",
    "\n",
    "# Carregar o modelo e os scalers/encoders se não foram carregados anteriormente\n",
    "if 'model' not in locals():\n",
    "    print(\"ℹ️ Carregando modelo pré-existente para detecção em tempo real...\")\n",
    "    # O TensorFlow irá carregar e usar a CPU.\n",
    "    model = load_model(MODEL_PATH)\n",
    "    exercise_encoder = np.load('exercise_encoder.npy', allow_pickle=True).item()\n",
    "    feature_scaler = np.load('feature_scaler.npy', allow_pickle=True).item()\n",
    "    print(\"✅ Modelo e scalers carregados.\")\n",
    "\n",
    "# Mapeamento para os rótulos de forma\n",
    "form_labels = ['Forma Correta', 'Forma Incorreta']\n",
    "\n",
    "# Inicializar o MediaPipe Pose para detecção em tempo real\n",
    "# 'static_image_mode=False' para melhor desempenho em streams de vídeo.\n",
    "# O MediaPipe automaticamente usa a CPU neste ambiente.\n",
    "pose_live_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Função de predição em tempo real\n",
    "def detect_exercise_form(frame, pose_detector, model, feature_scaler, exercise_encoder, form_labels):\n",
    "    \"\"\"\n",
    "    Processa um frame da webcam para detectar a pose e prever o exercício e a forma.\n",
    "    Args:\n",
    "        frame (numpy.ndarray): O frame de vídeo da webcam.\n",
    "        pose_detector (mp.solutions.pose.Pose): Objeto MediaPipe Pose inicializado (executando em CPU).\n",
    "        model (tensorflow.keras.Model): O modelo de ML treinado (inferência em CPU).\n",
    "        feature_scaler (MinMaxScaler): O scaler usado para normalizar as features.\n",
    "        exercise_encoder (LabelEncoder): O encoder usado para os rótulos de exercício.\n",
    "        form_labels (list): Lista de rótulos para a forma (e.g., ['Correto', 'Incorreto']).\n",
    "    Returns:\n",
    "        numpy.ndarray: O frame com as informações de pose e feedback.\n",
    "    \"\"\"\n",
    "    # Converter o frame para RGB para o MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Melhorar o desempenho (opcional): marcar a imagem como não gravável para passar por referência.\n",
    "    image_rgb.flags.writeable = False\n",
    "    \n",
    "    # Processar o frame para estimativa de pose (executa na CPU)\n",
    "    results = pose_detector.process(image_rgb)\n",
    "    \n",
    "    # Marcar a imagem como gravável novamente para desenhar as anotações\n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # Voltar para BGR para o OpenCV\n",
    "    \n",
    "    feedback_text = \"Nenhuma pose detectada.\"\n",
    "    text_color = (255, 255, 255) # Cor padrão branca para feedback\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Desenhar as landmarks da pose no frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_bgr,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Extrair features da pose detectada no frame atual\n",
    "        # Replicamos a lógica de extract_pose_features para o frame atual\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        try:\n",
    "            # Coletar as mesmas features que foram usadas no treinamento\n",
    "            live_features = np.array([\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                ),\n",
    "                calculate_angle(\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y],\n",
    "                    [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "                )\n",
    "            ])\n",
    "            \n",
    "            # Normalizar as features usando o scaler treinado\n",
    "            live_features_scaled = feature_scaler.transform(live_features.reshape(1, -1))\n",
    "            \n",
    "            # Fazer a predição com o modelo (inferência em CPU)\n",
    "            predictions = model.predict(live_features_scaled, verbose=0)\n",
    "            \n",
    "            # Interpretar as predições\n",
    "            exercise_probs = predictions[0][0]\n",
    "            form_probs = predictions[1][0]\n",
    "            \n",
    "            # Obter o exercício e a forma com maior probabilidade\n",
    "            predicted_exercise_idx = np.argmax(exercise_probs)\n",
    "            predicted_exercise = exercise_encoder.inverse_transform([predicted_exercise_idx])[0]\n",
    "            \n",
    "            predicted_form_idx = np.argmax(form_probs)\n",
    "            predicted_form = form_labels[predicted_form_idx]\n",
    "            \n",
    "            feedback_text = f\"Exercicio: {predicted_exercise} ({exercise_probs[predicted_exercise_idx]*100:.1f}%) | Forma: {predicted_form} ({form_probs[predicted_form_idx]*100:.1f}%)\"\n",
    "            \n",
    "            # Adicionar sugestão de correção se a forma for incorreta (exemplo genérico)\n",
    "            if predicted_form_idx == 1: # Se a forma for incorreta\n",
    "                text_color = (0, 0, 255) # Vermelho\n",
    "                feedback_text += \" -> Ajuste sua postura!\"\n",
    "            else:\n",
    "                text_color = (0, 255, 0) # Verde para correto\n",
    "\n",
    "        except Exception as e:\n",
    "            feedback_text = f\"Erro na predição: {e}\"\n",
    "            text_color = (0, 165, 255) # Laranja para erro\n",
    "            print(f\"Erro na detecção: {e}\")\n",
    "\n",
    "    # Exibir o feedback no frame\n",
    "    cv2.putText(image_bgr, feedback_text, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image_bgr\n",
    "\n",
    "# Loop da Webcam para Detecção em Tempo Real\n",
    "\n",
    "# Inicializar a captura de vídeo da webcam (0 é geralmente a câmera padrão)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Não foi possível acessar a webcam. Verifique se ela está conectada e não está em uso.\")\n",
    "else:\n",
    "    # Definir resolução da webcam para melhor qualidade\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    window_name = 'Detecao de Forma de Exercicios (CPU) - Pressione Q para Sair'\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    print(\"\\n✅ Webcam ativada. Pressione 'Q' para sair da janela de visualização.\")\n",
    "    # Exibir a hora atual conforme as instruções\n",
    "    current_utc_time = time.gmtime()\n",
    "    current_brasilia_time = time.localtime(time.time() - 3 * 3600) # UTC-3\n",
    "    print(f\"UTC: {time.strftime('%d/%m/%Y %H:%M:%S', current_utc_time)} (UTC)\")\n",
    "    print(f\"Brasília: {time.strftime('%d/%m/%Y %H:%M:%S', current_brasilia_time)} (UTC-3)\")\n",
    "    print(\"\\n⚠️ A performance pode ser limitada pela sua CPU. Para melhor experiência, considere GPUs dedicadas.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"❌ Falha ao ler frame da webcam.\")\n",
    "                break\n",
    "                \n",
    "            # Espelhar o frame para que a visualização seja mais intuitiva (como um espelho)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Chamar a função de detecção e obter o frame com feedback\n",
    "            output_frame = detect_exercise_form(frame, pose_live_detector, model, feature_scaler, exercise_encoder, form_labels)\n",
    "            \n",
    "            # Mostrar o frame na janela\n",
    "            cv2.imshow(window_name, output_frame)\n",
    "            \n",
    "            # Verificar se a janela foi fechada pelo usuário\n",
    "            if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "                \n",
    "            # Sair do loop se a tecla 'q' for pressionada\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        # Liberar os recursos da webcam e fechar todas as janelas do OpenCV\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"✅ Programa finalizado. Recursos liberados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128f15f-4027-40c7-811d-e9a1120a5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633fd4d-72ed-4d60-ab3f-a5f0f51d6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b9570-7776-43ec-ad67-eaa8fc55d0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
